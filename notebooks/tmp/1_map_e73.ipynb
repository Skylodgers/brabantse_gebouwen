{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0ed3d9-970b-4cd7-b1c2-052e114505e3",
   "metadata": {},
   "source": [
    "## Cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dbf0952-c2ca-4665-b3f1-840969562e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from edtf import text_to_edtf\n",
    "import datetime\n",
    "import shutil\n",
    "import requests\n",
    "from import_spec import * # package and number of records to import, see import_spec.py\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "lookup_path = '../../lookup/'\n",
    "source_path = '../../source/tg/'\n",
    "mappings_path = '../../mappings/'\n",
    "lookup_e73_uiid_id_path = lookup_path + 'record_ids_uuids/e73/'\n",
    "place_source_dir = '../../source/static/places/'\n",
    "transactions_dir = '../../source/static/tg_history/'\n",
    "save_path = '../../../digipolis-arches-shoku-pkg/source'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f346270-cb02-47e4-aa15-7453b087ed63",
   "metadata": {},
   "source": [
    "## Lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8249fff-21e3-4879-8bc2-4dcbb7bbf073",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_agents_df = pd.read_csv(lookup_path + 'aspace/as2arches_merged.csv')\n",
    "lookup_relation_df = pd.read_csv(lookup_path + 'isaar_relatie_types.csv')\n",
    "lookup_all_plus_one_df = pd.read_csv(lookup_path + 'aspace/as2arches_all_plus_one.csv')\n",
    "with open(lookup_path + 'aspace/replace_source_au_codes.csv') as csvfile:\n",
    "    replace_au_code_dict = dict(csv.reader(csvfile))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0688898-a733-4a4f-b2bb-c642cdfe2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(mappings_path + 'mappings_e73.json') as f:\n",
    "    multi_value_mappings = json.load(f)  \n",
    "\n",
    "with open(mappings_path + 'resource_models_%s.json' % (resource_model) ) as f:\n",
    "    resource_model_list = json.load(f)   \n",
    "\n",
    "with open(mappings_path + 'static.json') as f:\n",
    "    static_list = json.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed9924e-9aaa-42e6-ae4a-94ec1425fc90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af069a79-928f-4753-9f61-9b269a13f3c6",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "220cfa93-c1df-499b-a1bc-0292f69f597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ark_identifier_by_row_value(package, uuid):\n",
    "    return '%s/ark:/%s/%s' % (static_list[package]['ark_url'], static_list[package]['naan'], uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f41b1fef-ee8b-4636-a0e0-8b430008c8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ead_identifier_by_row_value(package, uuid):\n",
    "    return '%s%s' % (static_list[package]['ead_uri'], uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33518a92-bf2b-4740-a94d-b81e9e8cba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ark_identifier_df(source_df, uuid_column, package):\n",
    "    for idx, row in source_df.iterrows():\n",
    "        source_df.loc[idx, _args['column']] = '%s/ark:/%s/%s'  % (static_list[package]['ark_url'], static_list[package]['naan'], row[uuid_column])         \n",
    "    return source_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34071b93-9fd3-48a6-b04c-f9a1f75c5532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_column_df(source_df, column, args):\n",
    "    lookup_df = pd.read_csv(lookup_path + args['lookup_file'])\n",
    "    source_df[column] = source_df[args['match_source']].map(lookup_df.set_index(args['code'])[args['concept']])\n",
    "    return source_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3247280-3013-46a5-95f2-aaba0fd77127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_column_value(code, lookup_file, card, brocade_id, source_field):\n",
    "    lookup_df = pd.read_csv(lookup_path + lookup_file)\n",
    "    try:\n",
    "        concept_name = lookup_df[lookup_df['code'] == code]['concept'].iloc[0]\n",
    "        return concept_name\n",
    "    except:\n",
    "        #print('%s,%s,%s,%s,%s' % (card, brocade_id, source_field, code, lookup_file))\n",
    "        print(brocade_id, ',', code + ',NOT FOUND IN,' + lookup_file)\n",
    "        return code + ' NOT FOUND IN: ' + lookup_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63128f99-6fd0-4362-b5b8-69f27a600fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mapped_set(source_df, _package, _resource_model, _card):\n",
    "    if _package in ['pkg_lhps', 'pkg_lhph', 'pkg_lhbr']:\n",
    "        package = 'lh'\n",
    "    else:\n",
    "        package = _package.split('_')[1]     \n",
    "    \n",
    "    file_name = '%s/%s/%s/%s.csv' % (save_path, package, _resource_model, _card)\n",
    "    source_df.to_csv(file_name, index=False)    \n",
    "    del source_df\n",
    "    return 'Saved OK'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff01ed37-2072-4cc2-92f6-6d38f444130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constant_type(mapping, source_key):\n",
    "    # pull constants from a node mapping\n",
    "    for node_mapping in mapping['node_mappings']:\n",
    "        if node_mapping['from'] == source_key:\n",
    "            return node_mapping['constants']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f91b4600-d80c-4151-a0ec-9dcb4d380f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_default_dict(obj, card, source_value, source_field, model_class):\n",
    "\n",
    "    default_dict = {}\n",
    "    default_dict['ResourceID'] = obj['ResourceID']\n",
    "    default_dict['brocade.id'] = obj['brocade.id']\n",
    "    default_dict['card'] = card\n",
    "    default_dict['order'] = obj['order']\n",
    "    default_dict['source_field'] = source_field\n",
    "    default_dict['source_code'] =  source_value\n",
    "    \n",
    "    return default_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3769aef3-e894-4bd3-bcc0-7c1a872a5494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mark_type(mapping, source_key):\n",
    "\n",
    "    for node_mapping in mapping['node_mappings']:\n",
    "        if node_mapping['from'] == source_key:\n",
    "            return node_mapping['constants']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0173431-3758-49c9-be28-d0e612c2bfa1",
   "metadata": {},
   "source": [
    "## Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fe688e1-e408-4ec4-b08b-dd36b1cdaa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_keys(v):\n",
    "    group_key_list = []\n",
    "    for mapping in v['node_mappings']:\n",
    "        group_key_list.append(mapping['from'].split('.')[0])\n",
    "    group_key_list = list(dict.fromkeys(group_key_list) ) \n",
    "    return(group_key_list)\n",
    "\n",
    "def get_field_keys(v):\n",
    "    field_list = []\n",
    "    try:\n",
    "        for mapping in v['node_mappings']:\n",
    "            field_list.append(mapping['from'].split('.')[1])\n",
    "        return(field_list)\n",
    "    except:\n",
    "        print('Error in:', mapping)\n",
    "        \n",
    "def get_from_keys(v):\n",
    "    map_list = []\n",
    "    for mapping in v['node_mappings']:\n",
    "        map_list.append(mapping['from'])\n",
    "    return(map_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecbd81c-2a6f-484f-a7b2-e11455c14826",
   "metadata": {},
   "source": [
    "## Cards:\n",
    "* Object Types \n",
    "* Genre Types \n",
    "* Languages \n",
    "* Merkteken Types \n",
    "* Writing Material\n",
    "* ProductStadiums\n",
    "* Document status\n",
    "* Carrier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d9291-a631-4f46-a7d5-577335a9a693",
   "metadata": {},
   "source": [
    "# E73 Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e94666b-f844-408f-bdc5-b6f95eac3d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_type(dataset, mapping, source_key):\n",
    "\n",
    "    for node_mapping in mapping['node_mappings']:\n",
    "        if node_mapping['from'] == source_key:\n",
    "            return node_mapping['constants']['Title Type']\n",
    "lang_replace = {\n",
    "    \"hun\": \"Magyar (Hungarian) (language)\",    \n",
    "    \"afr\": \"Afrikaans (language)\",\n",
    "    \"dut\": \"Dutch (language)\",\n",
    "    \"eng\": \"English (language)\",\n",
    "    \"fre\": \"French (language)\",\n",
    "    \"ger\": \"German (language)\",\n",
    "    \"grc\": \"Ancient Greek (language)\",\n",
    "    \"gri\": \"Greek (language)\",\n",
    "    \"heb\": \"Hebrew (language)\",\n",
    "    \"ita\": \"Italian (language)\",\n",
    "    \"lat\": \"Latin (language)\",\n",
    "    \"noo\": \"Norwegian (language)\",\n",
    "    \"nor\": \"Norwegian (language)\",\n",
    "    \"por\": \"Portuguese (language)\",\n",
    "    \"spa\": \"Spanish (language)\",\n",
    "    \"zwe\": \"Swedish (language)\",\n",
    "    \"nds\": \"Low German (language)\",\n",
    "    \"mul\": \"Dutch (language)\",\n",
    "    \"und\": \"Dutch (language)\" \n",
    "}                \n",
    "def make_titles(source_list, resource_model, package, dataset, card):\n",
    "\n",
    "    title_dict = {}\n",
    "    title_list = []\n",
    "    mappings = multi_value_mappings[card]\n",
    "    \n",
    "    for title in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in title.keys():\n",
    "                if 'value' in title[node_mapping['from']][0].keys():\n",
    "                    title_dict['ResourceID'] = title['ResourceID']\n",
    "                    title_dict['brocade.id'] = title['brocade.id']\n",
    "                    title_dict['card'] = card\n",
    "                    title_dict['order'] = title['order']\n",
    "                    title_dict['source_field'] = node_mapping['from']\n",
    "                    _title = title[node_mapping['from']][0]['value'].replace('<i>', '').replace('</i>', '').replace('<I>', '').replace('</I>', '')#.replace('\\\"\\\"Inspanning\\\"\\\"', 'Inspanning')\n",
    "                    title_dict['Title'] = _title.replace('\"\"','\"')\n",
    "                    title_dict['Title Type'] = node_mapping['constants']['Title Type']\n",
    "                    if 'language' in title[node_mapping['from']][0].keys():\n",
    "                        title_dict['Title Language'] = title[node_mapping['from']][0]['language']\n",
    "        \n",
    "                    title_list.append(title_dict)\n",
    "                    #print(title['brocade.id'], title['order'], _title)\n",
    "                    title_dict = {}\n",
    "\n",
    "    source_df = pd.DataFrame(title_list)\n",
    "    source_df = source_df.replace({'Title Language': lang_replace})        \n",
    "    \n",
    "    \n",
    "    # save\n",
    "    if len(source_df) > 0:\n",
    "        save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "    #source_df.to_csv('%s_%s_Group_%s.csv' % (resource_model, card, package), index=False)\n",
    "\n",
    "    #dim_dict = {}\n",
    "    #dim_list = []\n",
    "   \n",
    "    return '%s saved: %s' % (card, len(source_df)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc2c90c-8e5b-4a88-98d7-809e438d5ab5",
   "metadata": {},
   "source": [
    "* Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7caf0237-02ba-4547-90d8-5a88da87ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e73_make_with_function_and_include_codes(source_list, resource_model, package, dataset, card):\n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    default_dict_items = {}\n",
    "    \n",
    "    for obj in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in obj.keys():\n",
    "                source_value = obj[node_mapping['from']][0]['value']\n",
    "                if 'include_codes' in node_mapping.keys(): \n",
    "                    if source_value in node_mapping['include_codes']:\n",
    "                        obj_dict.update(make_default_dict(obj, card, source_value, node_mapping['from'], 'e73'))\n",
    "                        if 'function' in node_mapping.keys():\n",
    "                            if node_mapping['function']['name'] == 'column_lookup': \n",
    "                                _type = lookup_column_value(obj[node_mapping['from']][0]['value'], \n",
    "                                            node_mapping['function']['args']['lookup_file'],\n",
    "                                            card, obj['brocade.id'], node_mapping['from'])\n",
    "                                obj_dict[node_mapping['to']] = _type                            \n",
    "                        obj_list.append(obj_dict)\n",
    "                        obj_dict = {}\n",
    "\n",
    "    source_df = pd.DataFrame(obj_list)\n",
    "    number_of_records = len(obj_list)\n",
    "    save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "    del source_df\n",
    "    del source_list\n",
    "    del obj\n",
    "    return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718d8dff-bc15-4d03-9774-611886fdd425",
   "metadata": {},
   "source": [
    "## Cards:\n",
    "* Keywords\n",
    "* CreationTimeSpans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "855017a1-57bb-417d-b82b-dbc9cf29b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plain_mappings(source_list, resource_model, package, dataset, card):\n",
    "    \n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    \n",
    "    for obj in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in obj.keys():\n",
    "                obj_dict['ResourceID'] = obj['ResourceID']\n",
    "                obj_dict['brocade.id'] = obj['brocade.id']\n",
    "                obj_dict['card'] = card\n",
    "                obj_dict['order'] = obj['order']\n",
    "                obj_dict['source_field'] = node_mapping['from']\n",
    "                \n",
    "                obj_dict[node_mapping['from']] = obj[node_mapping['from']][0]['value']\n",
    "                if 'constants' in node_mapping.keys():\n",
    "                    constants = get_constant_type(mappings, node_mapping['from'])\n",
    "                    for _const, _value in constants.items():\n",
    "                        obj_dict[_const] = _value                \n",
    "                obj_list.append(obj_dict)\n",
    "                obj_dict = {}\n",
    "    source_df = pd.DataFrame(obj_list)\n",
    "    \n",
    "    for node_mappings in mappings['node_mappings']:\n",
    "        obj_dict[node_mappings['from']] = node_mappings['to']        \n",
    "    dataset_df = source_df.rename(columns=obj_dict)\n",
    "    number_of_record = len(source_df)\n",
    "    \n",
    "    save_it = save_mapped_set(dataset_df, package, resource_model, card)\n",
    "    del source_df\n",
    "    del dataset_df\n",
    "    return '%s saved: %s' % (card, number_of_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dde73ff-6ab8-47ff-b82e-ca2c5bb7159c",
   "metadata": {},
   "source": [
    "## External Relations Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a40658c5-57b4-4338-9b98-59dbd2adfd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_external_relations_extra(source_list, resource_model, package, dataset, card):\n",
    "    \n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    _type = ''\n",
    "    \n",
    "    #raw_df = pd.DataFrame(source_list)\n",
    "    #raw_df.to_csv('out/' + resource_model + '_from_source_ext_ext.csv', index=False)\n",
    "    \n",
    "    \n",
    "    for obj in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in obj.keys():\n",
    "                obj_dict['ResourceID'] = obj['ResourceID']\n",
    "                obj_dict['brocade.id'] = obj['brocade.id']\n",
    "                obj_dict['card'] = card\n",
    "                obj_dict['order'] = obj['order']\n",
    "                obj_dict['source_field'] = node_mapping['from']\n",
    "                \n",
    "                if node_mapping['from'] == 'ere.urltype':    \n",
    "                    _prefix_type = lookup_column_value(obj[node_mapping['from']][0]['value'], \n",
    "                                    'url_prefix_lookup.csv',\n",
    "                                    card, obj['brocade.id'], node_mapping['from'])\n",
    "                    obj_dict['External Relation URL Prefix'] = _prefix_type\n",
    "\n",
    "                if node_mapping['from'] == 'doc.mpmtranscription':    \n",
    "                    _prefix_type = lookup_column_value(obj[node_mapping['from']][0]['type'], \n",
    "                                    'url_prefix_lookup.csv',\n",
    "                                    card, obj['brocade.id'], node_mapping['from'])\n",
    "                    obj_dict['External Relation URL Prefix'] = _prefix_type\n",
    "\n",
    "                if node_mapping['from'] == 'ow.link':    \n",
    "                    obj_dict['External Relation URL Prefix'] = 'https://rkd.nl/nl/explore/images/'\n",
    "\n",
    "                if 'function' in node_mapping.keys():\n",
    "                    if node_mapping['function']['name'] == 'column_lookup':\n",
    "                        source_value = obj[node_mapping['from']][0]['value']\n",
    "                        if source_value.endswith(':1'):\n",
    "                            lookup_value = source_value[:-2] \n",
    "                        else:\n",
    "                            lookup_value = source_value\n",
    "                        _type = lookup_column_value(lookup_value, \n",
    "                            node_mapping['function']['args']['lookup_file'],\n",
    "                            card, obj['brocade.id'], node_mapping['from'])\n",
    "                        obj_dict[node_mapping['to']] = _type\n",
    "                else:\n",
    "                    obj_dict[node_mapping['to']] = obj[node_mapping['from']][0]['value']\n",
    "                if 'constants' in node_mapping.keys():\n",
    "                    constants = get_constant_type(mappings, node_mapping['from'])\n",
    "                    for _const, _value in constants.items():\n",
    "                        obj_dict[_const] = _value \n",
    "                        \n",
    "                if _type != 'DO_NOT_MIGRATE':        \n",
    "                    obj_list.append(obj_dict)\n",
    "                obj_dict = {}\n",
    "    source_df = pd.DataFrame(obj_list)\n",
    "    \n",
    "    for node_mappings in mappings['node_mappings']:\n",
    "        obj_dict[node_mappings['from']] = node_mappings['to']        \n",
    "    dataset_df = source_df.rename(columns=obj_dict)\n",
    "    number_in_source_list = len(source_list)\n",
    "    number_in_dataset = len(dataset_df)\n",
    "\n",
    "    if number_in_dataset > 0:\n",
    "        save_it = save_mapped_set(dataset_df, package, resource_model, card)\n",
    "    del source_df\n",
    "    del dataset_df\n",
    "    return f'{card}: found in source: {number_in_source_list}, saved: {number_in_dataset}' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6f6f45-2bf5-4d4f-a378-ef182aabe608",
   "metadata": {},
   "source": [
    "## Correspondance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26de3372-fec6-4a97-b5ce-d601a5ffa0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_correspondence(source_list, resource_model, package, dataset, card):\n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    \n",
    "    out_df = pd.DataFrame()\n",
    "    \n",
    "    source_df = pd.DataFrame(source_list)\n",
    "    for idx, row in source_df.iterrows():\n",
    "        key_column = 'kenmerken.rubsender'\n",
    "        if key_column in source_df.columns:\n",
    "            if pd.notna(row[key_column]):\n",
    "                if isinstance(row[key_column], list): \n",
    "                    out_dict = {'ResourceID': row['ResourceID'], \n",
    "                        'brocade.id': row['brocade.id'], \n",
    "                        'card': row['card'],\n",
    "                        'order': row['order'],\n",
    "                        'Creation Actor Note': row[key_column][0]['value'],\n",
    "                        'Creation Actor Role': 'correspondents (correspondence writers)',                   \n",
    "                       }\n",
    "                    out_df = out_df.append(out_dict, ignore_index = True)\n",
    "                            \n",
    "        key_column = 'kenmerken.rubrecipient'\n",
    "        if key_column in source_df.columns:\n",
    "            if pd.notna(row[key_column]):\n",
    "                if isinstance(row[key_column], list): \n",
    "                    source_df.loc[idx, 'rubrecipient'] = row[key_column][0]['value']     \n",
    "\n",
    "                    out_dict = {'ResourceID': row['ResourceID'], \n",
    "                        'brocade.id': row['brocade.id'], \n",
    "                        'card': row['card'],\n",
    "                        'order': row['order'],\n",
    "                        'Creation Actor Note': row[key_column][0]['value'],\n",
    "                        'Creation Actor Role': 'recipients (people)',                   \n",
    "                       }\n",
    "\n",
    "                out_df = out_df.append(out_dict, ignore_index = True)\n",
    "\n",
    "    dataset_df = out_df\n",
    "    \n",
    "    save_it = save_mapped_set(dataset_df, package, resource_model, card)\n",
    "    return '%s saved: %s' % (card, len(dataset_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b11d830-48fe-41a4-a2cf-aa20345a6bbf",
   "metadata": {},
   "source": [
    "## Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27290617-0ef9-4c81-91a2-f1bb4c7dabf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def make_descriptions(source_list, resource_model, package, dataset, card):\n",
    "\n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    \n",
    "    for obj in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in obj.keys():\n",
    "                obj_dict['ResourceID'] = obj['ResourceID']\n",
    "                obj_dict['brocade.id'] = obj['brocade.id']\n",
    "                obj_dict['card'] = card\n",
    "                obj_dict['order'] = obj['order']\n",
    "                obj_dict['source_field'] = node_mapping['from']\n",
    "                obj_dict[node_mapping['to']] = obj[node_mapping['from']][0]['value']\n",
    "                if 'constants' in node_mapping.keys():\n",
    "                    constants = get_constant_type(mappings, node_mapping['from'])\n",
    "                    for _const, _value in constants.items():\n",
    "                        obj_dict[_const] = _value                \n",
    "                obj_list.append(obj_dict)\n",
    "                obj_dict = {}\n",
    "    source_df = pd.DataFrame(obj_list)\n",
    "    #source_df.to_csv('out/%s_%s_Group_%s.csv' % (resource_model, card, package), index=False)\n",
    "    \n",
    "    for node_mappings in mappings['node_mappings']:\n",
    "        obj_dict[node_mappings['from']] = node_mappings['to']        \n",
    "    dataset_df = source_df.rename(columns=obj_dict)\n",
    "\n",
    "    \n",
    "    save_it = save_mapped_set(dataset_df, package, resource_model, card)\n",
    "\n",
    "    return '%s saved: %s' % (card, len(dataset_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a013e947-708a-4c66-9b0a-eee14a8c0f07",
   "metadata": {},
   "source": [
    "## Legacy Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "858426f8-0350-423d-ad69-ad13dc1f99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_legacy_info(source_list, resource_model, package, dataset, card):\n",
    "    usr_dict = {}\n",
    "    usr_list = []\n",
    "    mapping = multi_value_mappings[card]\n",
    "    \n",
    "    for usr in source_list:\n",
    "        for node_mapping in mapping['node_mappings']:\n",
    "            if node_mapping['from'] in usr.keys():\n",
    "                if isinstance(usr[node_mapping['from']], list):\n",
    "                    usr_dict['ResourceID'] = usr['ResourceID']\n",
    "                    usr_dict['card'] = card\n",
    "                    usr_dict['order'] = usr['order']\n",
    "                    usr_dict['source_field'] = node_mapping['from']\n",
    "                    source_value = usr[node_mapping['from']][0]['value']\n",
    "                    usr_dict[node_mapping['to']] = source_value    \n",
    "                    if 'constants' in node_mapping.keys():\n",
    "                        constants = get_constant_type(mapping, node_mapping['from'])\n",
    "                        for _const, _value in constants.items():\n",
    "                            usr_dict[_const] = _value\n",
    "                    usr_list.append(usr_dict)\n",
    "                    usr_dict = {}\n",
    "\n",
    "    source_df = pd.DataFrame(usr_list)\n",
    "    \n",
    "    # save\n",
    "    #source_df.to_csv('out/' + dataset + '_legacy_merged.csv', index=False)\n",
    "    save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "    return '%s saved: %s' % (card, len(source_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9262b2d3-f682-43af-b169-5508cb9c65b0",
   "metadata": {},
   "source": [
    "## Legacy Information Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b6daa55-9eda-4573-9b7b-ba8b01783d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_legacy_info_merged(source_list, resource_model, package, dataset, card):\n",
    "\n",
    "    mapped_df = pd.DataFrame(source_list)\n",
    "    mappings = multi_value_mappings[card]\n",
    "    \n",
    "    for column in mapped_df.columns:\n",
    "        for idx, row in mapped_df.iterrows():\n",
    "            if isinstance(row[column], list): \n",
    "                mapped_df.loc[idx, column] = row[column][0]['value']\n",
    "    m_dict = {}\n",
    "    for mapping in mappings['node_mappings']:\n",
    "        m_dict[mapping['from']] = mapping['to']\n",
    "    mapped_df = mapped_df.rename(columns=m_dict)    \n",
    "    mapped_df['Legacy Information Type'] = 'groepsbeschrijving'\n",
    "    mapped_df['Legacy Information Published'] = 'False'\n",
    "\n",
    "    for idx, row in mapped_df.iterrows():\n",
    "        if 'number' in mapped_df.columns:\n",
    "            if pd.notna(row['number']):    \n",
    "                mapped_df.loc[idx, 'Legacy Information Value'] = '%s %s' % (row['number'],row['type'])\n",
    "            else:\n",
    "                mapped_df.loc[idx, 'number'] = '1'\n",
    "                mapped_df.loc[idx, 'Legacy Information Value'] = '%s %s' % ('1',row['type'])\n",
    "    \n",
    "    mapped_df = mapped_df[['ResourceID', 'brocade.id', 'card', 'order', 'number', \n",
    "                               'type', 'Legacy Information Value', 'Legacy Information Type', 'Legacy Information Published']]  \n",
    "\n",
    "    mapped_df = mapped_df.loc[(mapped_df['type'] == 'brgr') | (mapped_df['number'] != \"1\")]\n",
    "    \n",
    "    #mapped_df.to_csv('out/' + dataset + '_legacy_merged.csv', index=False)\n",
    "    save_it = save_mapped_set(mapped_df, package, resource_model, card)\n",
    "\n",
    "    number_of_records = len(mapped_df)\n",
    "    del mapped_df    \n",
    "\n",
    "    return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce700f09-a605-4823-844e-5ce23f488a55",
   "metadata": {},
   "source": [
    "## Associated Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cd2213e-6802-4ca7-8cda-f09d5c7ee0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_aspace_archives_df = pd.read_csv(lookup_path + 'aspace/as2arches_archiveobject.csv')\n",
    "lookup_rub_isad_df = pd.read_csv(lookup_path + 'rub_isad_internal_relations.csv')\n",
    "\n",
    "def make_associated_archive(source_list, resource_model, package, dataset, card):\n",
    "    \n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    \n",
    "    for obj in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in obj.keys():\n",
    "                if obj['order'] >= '2':\n",
    "                    obj_dict['ResourceID'] = obj['ResourceID']\n",
    "                    obj_dict['brocade.id'] = obj['brocade.id']\n",
    "                    obj_dict['card'] = card\n",
    "                    obj_dict['order'] = obj['order']\n",
    "                    obj_dict['source_field'] = node_mapping['from']\n",
    "                    obj_dict[node_mapping['from']] = obj[node_mapping['from']][0]['value']           \n",
    "                    obj_list.append(obj_dict)\n",
    "                    obj_dict = {}\n",
    "\n",
    "    source_df = pd.DataFrame(obj_list)\n",
    "\n",
    "    \n",
    "    if (len(source_df) > 0):\n",
    "        source_df['isaad'] = source_df[node_mapping['from']].map(lookup_rub_isad_df.set_index('code')['isad'])  \n",
    "        source_df['isaad'] = source_df['isaad'] + ':1' \n",
    "        source_df['Associated Archive'] = source_df['isaad'].map(lookup_aspace_archives_df.set_index('archesID')['json'])\n",
    "\n",
    "    #source_df.to_csv('out/' + dataset + '_ass_arch.csv')\n",
    "\n",
    "#    for node_mappings in mappings['node_mappings']:\n",
    "#        obj_dict[node_mappings['from']] = node_mappings['to']        \n",
    "#    dataset_df = source_df.rename(columns=obj_dict)\n",
    "    dataset_df = source_df\n",
    "    if len(dataset_df) > 0:\n",
    "        save_it = save_mapped_set(dataset_df, package, resource_model, card)\n",
    "    return '%s saved: %s' % (card, len(dataset_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36772653-9ad8-425b-b590-b4652bbdcf9a",
   "metadata": {},
   "source": [
    "## E73 Material records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b1d7632-1551-473a-b20d-9b8dd8e96ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_material_records(source_list, resource_model, package, dataset, card):\n",
    "    usr_dict = {}\n",
    "    usr_list = []\n",
    "    mapping = multi_value_mappings[card]\n",
    "    related_resource_dict = {}\n",
    "    related_resource_list = []\n",
    "\n",
    "    \n",
    "    for usr in source_list:\n",
    "        for node_mapping in mapping['node_mappings']:\n",
    "            if node_mapping['from'] in usr.keys():\n",
    "                n=1\n",
    "                for mat_rec_uuid in usr[node_mapping['from']]:\n",
    "                    usr_dict['ResourceID'] = usr['ResourceID']\n",
    "                    usr_dict['brocade.id'] = usr['brocade.id']\n",
    "                    usr_dict['card'] = card\n",
    "                    usr_dict['order'] = n\n",
    "                    usr_dict['source_field'] = node_mapping['from']                    \n",
    "                    relation = '[{\"resourceId\": \"%s\", \"ontologyProperty\": \"\", \"resourceXresourceId\": \"%s\", \"inverseOntologyProperty\": \"\"}]' % (mat_rec_uuid, uuid.uuid4())\n",
    "                    usr_dict[node_mapping['to']] = relation\n",
    "                    n+=1\n",
    "                    usr_list.append(usr_dict)\n",
    "                    usr_dict = {}\n",
    "                    \n",
    "                    related_resource_dict['resourceinstanceidfrom'] = usr['ResourceID']\n",
    "                    related_resource_dict['resourceinstanceidto'] = mat_rec_uuid\n",
    "                    related_resource_dict['relationshiptype'] = 'is related to'\n",
    "                    related_resource_dict['datestarted'] = ''\n",
    "                    related_resource_dict['dateended'] = ''\n",
    "                    related_resource_dict['notes'] = ''\n",
    "                \n",
    "                    related_resource_list.append(related_resource_dict)\n",
    "                    related_resource_dict = {}\n",
    "                    \n",
    "                    \n",
    "\n",
    "    source_df = pd.DataFrame(usr_list)\n",
    "    related_resource_df = pd.DataFrame(related_resource_list)\n",
    "\n",
    "    \n",
    "    # save\n",
    "    save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "    save_it = save_mapped_set(related_resource_df, package, resource_model, 'RelatedResources')\n",
    "    \n",
    "    return '%s saved: %s' % (card, len(source_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49480f78-a1d0-445b-ba83-f76f12518a4d",
   "metadata": {},
   "source": [
    "## IRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a60abbf7-af10-449d-841b-ad2e4a7d7cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'au::28114:2'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deleted_extensions = {\n",
    "   \"AM\": True, \"AMV\": True, \"BB\": True, \"BBV\": True, \"BH\": True, \"BHV\": True, \"BI\": True, \"BIV\": True, \"CO\": True,\n",
    "    \"COV\": True, \"EM\": True, \"EMV\": True, \"FB\": True, \"FBV\": True, \"GB\": True, \"GBV\": True, \"GEOB\": True, \"GEON\": True,\n",
    "    \"GR\": True, \"GRV\": True, \"IS\": True, \"ISV\": True, \"KV\": True, \"LE\": True, \"LEV\": True, \"LLVL\": True, \"LLVO\": True,\n",
    "    \"LV\": True, \"LVLL\": True, \"MI\": True, \"MIV\": True, \"ORG\": True, \"OVK\": True, \"OVL\": True, \"OVLL\": True, \"OVZ\": True, \"PM\": True,\n",
    "    \"PMV\": True, \"PR\": True, \"PRV\": True, \"PS\": True, \"PSV\": True, \"RE\": True, \"REV\": True, \"ROG\": True, \"SC\": True, \"SCV\": True, \n",
    "    \"UIE\": True, \"UIR\": True, \"ZO\": True,    \n",
    "}\n",
    "\n",
    "def split_extension(id):\n",
    "    parts = id.split(\":\")\n",
    "    ext = parts[-1]\n",
    "    return ext.split(\".\")[0]\n",
    "\n",
    "def clean_up_rel(id):\n",
    "    if \":N:\" in id:\n",
    "        id = id.replace(\":N:\", \"::\")\n",
    "    if id.endswith(':N'):\n",
    "        id = id[:-2]\n",
    "    if \":1.1\" in id:\n",
    "        id = id.replace(\":1.1\", \":1\")\n",
    "    \n",
    "    ext = split_extension(id)\n",
    "    if ext in deleted_extensions:\n",
    "        parts = id.split(\"::\")\n",
    "        id = \"::\".join(parts[0:2])\n",
    "        \n",
    "    return id\n",
    "\n",
    "clean_up_rel('au::28114:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1591207c-c734-4ed5-b956-a1f0cc3a831e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'au_code': 'au::28114:2', 'g_code': 'au::28114g:2'}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def fix_codes(au_code):\n",
    "    \n",
    "    f_dict = {}\n",
    "    au_split = au_code.split(':')\n",
    "    g_code = au_code\n",
    "\n",
    "    if len(au_split) == 3:\n",
    "        new_au_code = au_code + ':1'\n",
    "        new_g_code = g_code + 'g:1'\n",
    "    \n",
    "    elif len(au_split) == 4:\n",
    "        new_au_code = au_code\n",
    "        new_g_code = '%s::%sg:%s' % (au_split[0], au_split[2], au_split[3])\n",
    "    \n",
    "    elif len(au_split) == 5:\n",
    "        new_au_code = au_code\n",
    "        new_g_code = '%s::%sg:%s:%s' % (au_split[0], au_split[2], au_split[3], au_split[4])\n",
    "    \n",
    "    elif len(au_split) == 6:\n",
    "        if ':::' in au_code:\n",
    "            new_au_code = au_code.replace(':::', ':1::')\n",
    "            new_g_code = g_code.replace(':::', 'g:1::')\n",
    "        elif ':1:N:' in au_code:   \n",
    "            new_au_code = au_code\n",
    "            new_g_code = g_code.replace(':1:N:', 'g:1:N:')\n",
    "        elif ':2:N:' in au_code: \n",
    "            new_au_code = au_code\n",
    "            new_g_code = g_code.replace(':2:N:', 'g:2:N:')\n",
    "        elif ':2::' in au_code: \n",
    "            new_au_code = au_code\n",
    "            new_g_code = g_code.replace(':2::', 'g:2::')\n",
    "\n",
    "        elif ':1::' in au_code:\n",
    "            new_g_code = g_code.replace(':1::', 'g:1::')\n",
    "            new_au_code = au_code\n",
    "            \n",
    "    elif au_code.endswith(':1'):\n",
    "        new_g_code = g_code[:-2] + 'g:1'\n",
    "        new_au_code = au_code\n",
    "    else:\n",
    "        new_g_code = g_code\n",
    "        new_au_code = au_code\n",
    "    \n",
    "    try:\n",
    "        f_dict['au_code'] = clean_up_rel(new_au_code)\n",
    "        f_dict['g_code'] = clean_up_rel(new_g_code)\n",
    "        \n",
    "    except:\n",
    "        f_dict['au_code'] = 'nope'\n",
    "        f_dict['g_code'] = 'nope'\n",
    "        print('no fixed au or g code')\n",
    "    \n",
    "    return f_dict\n",
    "\n",
    "bulle = fix_codes('au::28114:2')\n",
    "print(bulle)\n",
    "print(len('au::28114:2'.split(':')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f2e3352-9d50-406c-ba86-17f39c009850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_au_codes(dataset_df, package, resource_model, card, mappings):\n",
    "    node_mapping_dict = {}  \n",
    "    \n",
    "    ## \n",
    "    ## all_au_df = all records that matches on au_code\n",
    "    ## g_df = all records that matches on the g_code\n",
    "    ## au_df = all_au_df - g_df\n",
    "    ##\n",
    "    ## if card == \"e73_CreationActors\":\n",
    "    ##    out_df = g_df + au_df\n",
    "    ## else:\n",
    "    ##   out_df = all_au   \n",
    "        \n",
    "    # g codes\n",
    "    g_df = dataset_df.copy()\n",
    "    g_df['ire.widget'] = g_df['g_code'].map(lookup_agents_df.set_index('archesID')['json'])\n",
    "    g_df['ire.identifier'] = g_df['g_code'].map(lookup_agents_df.set_index('archesID')['ark'])\n",
    "    g_df = g_df[g_df['ire.widget'].notnull()]\n",
    "    g_df['matched on'] = 'g_code'    \n",
    "    print()\n",
    "    print('g_df =', len(g_df))\n",
    "    \n",
    "    # au codes\n",
    "    all_au_df = dataset_df.copy()\n",
    "    all_au_df['matched on'] = 'au_code'\n",
    "    all_au_df['ire.widget'] = all_au_df['au_code'].map(lookup_agents_df.set_index('archesID')['json'])\n",
    "    all_au_df['ire.identifier'] = all_au_df['au_code'].map(lookup_agents_df.set_index('archesID')['ark'])\n",
    "    print('all_au_df =', len(all_au_df))\n",
    "\n",
    "    ## au codes minus g codes\n",
    "    #resource_ids_in_g = g_df['ResourceID']    \n",
    "\n",
    "    ## Drop rows from all_au_df where ResourceID is in g_df\n",
    "    #au_df = all_au_df[~all_au_df['ResourceID'].isin(resource_ids_in_g)]\n",
    "    #print('au_df (all_au_df - g_df) =', len(au_df))\n",
    "    \n",
    "    xmerged_df = pd.merge(all_au_df, g_df[['ResourceID', 'au_code']], on=['ResourceID', 'au_code'], how='left', indicator='is_in_g_df')\n",
    "\n",
    "    # Filter out the rows that are also present in g_df\n",
    "    au_df = xmerged_df[xmerged_df['is_in_g_df'] == 'left_only']\n",
    "\n",
    "    # Drop the indicator column as it's no longer needed\n",
    "    #au_df.drop(columns=['is_in_g_df'], inplace=True)\n",
    "    au_df = au_df.drop(columns=['is_in_g_df'])\n",
    "    print('au_df (all_au_df - g_df) =', len(au_df))\n",
    "\n",
    "    if card == \"e73_CreationActors\" and len(g_df) > 0:\n",
    "            print(card, 'has Gs, g_df + au_df saved')\n",
    "            merged_df = pd.concat([g_df, au_df])\n",
    "            #merged_df.to_csv(f'out/{resource_model}_{card}_au_and_g.csv', index=False)\n",
    "    else:\n",
    "        print(card, 'has NO Gs, all_au_df saved')\n",
    "        merged_df = all_au_df\n",
    "        #merged_df.to_csv(f'out/{resource_model}_{card}_au_all.csv', index=False)\n",
    "        \n",
    "\n",
    "    merged_df['ire.role'] = merged_df['ire.role'].map(lookup_relation_df.set_index('code')['concept'])        \n",
    "\n",
    "    un_matched_df = merged_df[merged_df['ire.widget'].isna()]\n",
    "        \n",
    "    # map colomn names\n",
    "    for node_mappings in mappings['node_mappings']:\n",
    "        node_mapping_dict[node_mappings['from']] = node_mappings['to']        \n",
    "    merged_df = merged_df.rename(columns=node_mapping_dict)\n",
    "        \n",
    "    #matched_df.to_csv(f'out/{package}_{resource_model}_{card}_matched.csv', index=False)\n",
    "    if len(un_matched_df) > 0:\n",
    "        un_matched_df.to_csv(f'out/unmatched_{package}_{resource_model}_{card}.csv', index=False)\n",
    "    return merged_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "506518dc-6cac-4739-9887-48b6809e5317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_matches(_dataset):\n",
    "    \n",
    "    for idx, row in _dataset.iterrows():\n",
    "        if pd.isna(_dataset.loc[idx,'ire.isaar.id']) is False:\n",
    "            mumps_response = requests.get('https://hub3.lh.delving.io/api/sync/resolve/%s?format=mumps' % (row['ire.isaar.id']))\n",
    "            _dataset.loc[idx, 'MUMPS'] = str(mumps_response.status_code)\n",
    "            mumps_response = requests.get('https://hub3.lh.delving.io/api/sync/resolve/%s?format=mumps' % (row['ire.isaar.id'][:-2]))\n",
    "            _dataset.loc[idx, 'MUMPS_NO_:1'] = str(mumps_response.status_code)\n",
    "    \n",
    "    _dataset['agent_type_in_all'] = _dataset['ire.isaar.id'].map(lookup_all_plus_one_df.set_index('brocadeID')['type'])\n",
    "    _dataset['place_type_in_all'] = _dataset['place_au_code'].map(lookup_all_plus_one_df.set_index('brocadeID')['type'])\n",
    "\n",
    "    return _dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e12b1ae-0a3a-4ea5-bb64-70b97c576237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ire_record(ire):\n",
    "    \n",
    "    fixed_codes_dict = {}\n",
    "    m_dict = {}\n",
    "    \n",
    "    m_dict['ResourceID'] = ire['ResourceID']\n",
    "    m_dict['brocade.id'] = ire['brocade.id']\n",
    "    m_dict['museum'] = package.split('_')[1]            \n",
    "    m_dict['dataset'] = ire['dataset']\n",
    "    m_dict['resource_model'] = resource_model            \n",
    "    m_dict['card'] = card\n",
    "    m_dict['ire.isaar.uuid'] = ire['ire.isaar'][0]['uuid']                       \n",
    "    m_dict['source_ire_type'] = ire['ire.type'][0]['value']\n",
    "    m_dict['ire.role'] = ire['ire.type'][0]['value']        \n",
    "    source_code = ire['ire.isaar'][0]['value']            \n",
    "    m_dict['source_code'] = source_code            \n",
    "            \n",
    "    # Normalize the source code for +g, +:1 etc\n",
    "    #replace_source_au_codes.csv\n",
    "    \n",
    "    if source_code in replace_au_code_dict.keys():\n",
    "        use_code = replace_au_code_dict[source_code]\n",
    "        #print(source_code, use_code)\n",
    "    else:\n",
    "        use_code = source_code\n",
    "    \n",
    "    fixed_codes_dict['fixed_codes'] = fix_codes(use_code)\n",
    "        \n",
    "    m_dict['au_code'] = fixed_codes_dict['fixed_codes']['au_code']\n",
    "    m_dict['g_code'] = fixed_codes_dict['fixed_codes']['g_code']\n",
    "            \n",
    "    if 'ire.markdown' in ire.keys():\n",
    "        m_dict['ire.markdown'] = ire['ire.markdown'][0]['value']\n",
    "    else:\n",
    "        m_dict['ire.markdown'] = None\n",
    "\n",
    "    return m_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c20b31-c001-476c-9252-ac7f1edc45b5",
   "metadata": {},
   "source": [
    "## Plain IRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "666ec7ca-eba9-4205-9660-e8b4f85b6ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plain_ire(source_list, resource_model, package, dataset, card, model_class):\n",
    "\n",
    "    node_mapping_dict = {}\n",
    "    isaar_list = []\n",
    "    card = card\n",
    "    mappings = multi_value_mappings[card]\n",
    "    \n",
    "    for ire in source_list:\n",
    "        if ire['ire.type'][0]['value'] in mappings['include_codes']: \n",
    "            isaar_list.append(make_ire_record(ire))        \n",
    "\n",
    "    dataset_df = pd.DataFrame(isaar_list)\n",
    "\n",
    "    if len(dataset_df) > 0:\n",
    "        matched_df = match_au_codes(dataset_df, package, resource_model, card, mappings)\n",
    "        matched_df.drop(matched_df.loc[matched_df['source_code']=='au::113865'].index, inplace=True)\n",
    "        save_it = save_mapped_set(matched_df, package, resource_model, card)\n",
    "\n",
    "        #return '%s saved: %s' % (card, len(dataset_df))   \n",
    "        return '%s saved: %s' % (card, len(matched_df))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844e4b7-eb4e-4346-a8cb-70332178f96f",
   "metadata": {},
   "source": [
    "## Make exception IREs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f6a49ea-333d-4312-8b79-5e6e90623f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_exception_ire(source_list, resource_model, package, dataset, card, model_class):\n",
    "\n",
    "    node_mapping_dict = {}\n",
    "    isaar_list = []\n",
    "    card = card\n",
    "    mappings = multi_value_mappings[card]\n",
    "    \n",
    "    for ire in source_list:\n",
    "        if ire['ire.type'][0]['value'] in mappings['include_codes'].keys():\n",
    "            au_type = ire['ire.type'][0]['value']\n",
    "            if ire['dataset'] in mappings['include_codes'][au_type]:\n",
    "                isaar_list.append(make_ire_record(ire))        \n",
    "\n",
    "    dataset_df = pd.DataFrame(isaar_list)\n",
    "    \n",
    "    if len(dataset_df) > 0:\n",
    "        matched_df = match_au_codes(dataset_df, package, resource_model, card, mappings)\n",
    "        matched_df.drop(matched_df.loc[matched_df['source_code']=='au::113865'].index, inplace=True)\n",
    "        save_it = save_mapped_set(matched_df, package, resource_model, card)\n",
    "\n",
    "        fixed_codes_dict = {}\n",
    "        #return '%s saved: %s' % (card, len(dataset_df))\n",
    "        return '%s saved: %s' % (card, len(matched_df))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c928fa-9f80-4e50-a725-7cbc9f331ae5",
   "metadata": {},
   "source": [
    "## Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c3e76b8-d0b7-456b-9882-8141418ffd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_constant(card, _key, _constant):\n",
    "    for item in multi_value_mappings[card]['node_mappings']:\n",
    "        if item['from'] == _key:\n",
    "            return item['constants'][0][_constant]\n",
    "\n",
    "def make_annotations(source_list, resource_model, package, dataset, card, model_class):\n",
    "\n",
    "    anno_dict = {}\n",
    "    anno_list = []\n",
    "    mappings = multi_value_mappings[card]\n",
    "\n",
    "    for anno in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in anno.keys():\n",
    "                source_value = anno[node_mapping['from']][0]['value']\n",
    "                if 'include_codes' in node_mapping.keys(): \n",
    "                    if source_value in node_mapping['include_codes']:\n",
    "                        anno_dict.update(make_default_dict(anno, card, source_value, node_mapping['from'], model_class))\n",
    "                        if anno[node_mapping['from']][0]['value'] == 'acai':\n",
    "                            anno_dict['Annotation'] = 'Alfabetische index'\n",
    "                        else:\n",
    "                            anno_dict['Annotation'] = anno[node_mapping['from']][0]['value']\n",
    "                        if 'constants' in node_mapping.keys():\n",
    "                            constants = get_mark_type(mappings, node_mapping['from'])\n",
    "                            for _type, _value in constants.items():\n",
    "                                anno_dict[_type] = _value\n",
    "                else: # no include codes\n",
    "                    anno_dict.update(make_default_dict(anno, card, source_value, node_mapping['from'], model_class))\n",
    "                    anno_dict['Annotation'] = anno[node_mapping['from']][0]['value']\n",
    "                    if 'constants' in node_mapping.keys():\n",
    "                        constants = get_mark_type(mappings, node_mapping['from'])\n",
    "                        for _type, _value in constants.items():\n",
    "                            anno_dict[_type] = _value\n",
    "                    \n",
    "                                \n",
    "        \n",
    "        anno_list.append(anno_dict)\n",
    "        anno_dict = {}\n",
    "\n",
    "    source_df = pd.DataFrame(anno_list)\n",
    "    #source_df.sort_values(by=['ResourceID', 'card', 'order'])\n",
    "    \n",
    "    # save\n",
    "    save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "    return '%s saved: %s' % (card, len(source_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecff07e-ffdf-4fd7-9f8c-317c4653a7a7",
   "metadata": {},
   "source": [
    "## External Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03f28d63-86e2-4351-bf1e-0faf61a8eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_external_relations(source_list, resource_model, package, dataset, card):\n",
    "    mappings = multi_value_mappings[card]\n",
    "    include_codes_list = mappings['include_codes']\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    default_dict_items = {}\n",
    "\n",
    "    #raw_df = pd.DataFrame(source_list)    \n",
    "    #raw_df.to_csv('out/' + resource_model + '_externals_raw.csv')\n",
    "    \n",
    "    for obj in source_list:\n",
    "        include_value = obj['ere.type'][0]['value']\n",
    "        if include_value in include_codes_list:\n",
    "        \n",
    "            #print(include_value)\n",
    "            obj_dict['ResourceID'] = obj['ResourceID']\n",
    "            obj_dict['brocade.id'] = obj['brocade.id']\n",
    "            obj_dict['card'] = obj['card']\n",
    "            obj_dict['order'] = obj['order']\n",
    "            for node_mapping in mappings['node_mappings']:\n",
    "                if node_mapping['from'] in obj.keys():\n",
    "                    if node_mapping['from'] == 'ere.url' or node_mapping['from'] == 'ere.markdown':\n",
    "                        if obj[node_mapping['from']][0]['value'].startswith(\"http\"):\n",
    "                            url = obj[node_mapping['from']][0]['value']\n",
    "                            url_splitted = obj[node_mapping['from']][0]['value'].split('/') \n",
    "                            obj_dict['External Relation URL Prefix'] = url.replace(url_splitted[-1], '')\n",
    "                            obj_dict[node_mapping['to']] = url_splitted[-1]\n",
    "                            #print(obj['brocade.id'], obj[node_mapping['from']][0]['value'], url_splitted[-1], url.replace(url_splitted[-1], ''))\n",
    "                            \n",
    "\n",
    "                        else:    \n",
    "                            obj_dict[node_mapping['to']] = obj[node_mapping['from']][0]['value']                        \n",
    "                        \n",
    "                    if node_mapping['from'] == 'ere.urltype':                        \n",
    "                        exclude_list = ['url', 'purl']\n",
    "                        \n",
    "                        if obj[node_mapping['from']][0]['value'] not in exclude_list:\n",
    "                            _prefix_type = lookup_column_value(obj[node_mapping['from']][0]['value'], \n",
    "                                    'url_prefix_lookup.csv',\n",
    "                                    card, obj['brocade.id'], node_mapping['from'])\n",
    "                            obj_dict['External Relation URL Prefix'] = _prefix_type\n",
    "\n",
    "                    \n",
    "                    if 'function' in node_mapping.keys():\n",
    "                        if node_mapping['function']['name'] == 'column_lookup': \n",
    "                            _type = lookup_column_value(obj[node_mapping['from']][0]['value'], \n",
    "                                        node_mapping['function']['args']['lookup_file'],\n",
    "                                        card, obj['brocade.id'], node_mapping['from'])\n",
    "                            obj_dict[node_mapping['to']] = _type\n",
    "\n",
    "                    if 'constants' in node_mapping.keys():\n",
    "                        constants = get_mark_type(mappings, node_mapping['from'])\n",
    "                        for _const, _value in constants.items():\n",
    "                            obj_dict[_const] = _value        \n",
    "                    \n",
    "            obj_list.append(obj_dict)\n",
    "            obj_dict = {}\n",
    "            #print(obj_list)\n",
    "                    \n",
    "    source_df = pd.DataFrame(obj_list)\n",
    "    number_of_records = len(obj_list)\n",
    "    save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "    del source_df\n",
    "    del source_list\n",
    "    del obj\n",
    "    return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921ba85-2f16-4258-be06-60e688fe41d8",
   "metadata": {},
   "source": [
    "## Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9003c690-7de8-4885-b3d1-2f8d95302dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_single(source_list, resource_model, package, dataset, card):\n",
    "    \n",
    "#    single_df = pd.DataFrame(source_list)\n",
    "#    single_df.to_csv('out/' + package + 'single.csv', index=False)\n",
    "\n",
    "    mappings = multi_value_mappings[card]\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    \n",
    "    for obj in source_list:\n",
    "        obj_dict['ResourceID'] = obj['ResourceID']\n",
    "        obj_dict['brocade.id'] = obj['brocade.id'][0]['value']    \n",
    "        obj_dict['card'] = 'AA'\n",
    "        obj_dict['order'] = '0'\n",
    "        if 'mpm' in obj['brocade.type'][0]['value']:\n",
    "            obj_dict['Keeper'] = 'Museum Plantin-Moretus'\n",
    "        elif 'lh' in obj['brocade.type'][0]['value']:\n",
    "            obj_dict['Keeper'] = 'Letterenhuis'\n",
    "        elif 'rub' in obj['brocade.type'][0]['value']:\n",
    "            obj_dict['Keeper'] = 'Rubenianum'\n",
    "\n",
    "        obj_dict['Object Number'] = obj['brocade.id'][0]['value']    \n",
    "        obj_dict['Object Number Type'] = 'record identifiers'\n",
    "\n",
    "        obj_dict['Object Identifier'] = add_ark_identifier_by_row_value(package, obj['ResourceID'])\n",
    "        obj_dict['Object Identifier Type'] = 'object identifier'\n",
    "\n",
    "        \n",
    "        \n",
    "        record_type = ''\n",
    "        if package == 'pkg_rub' and obj['brocade.subtype'][0]['value'] == 'gr':\n",
    "            \n",
    "            if resource_model == 'Foto':\n",
    "                record_type = \"Foto's\"\n",
    "            elif resource_model == 'Brief':\n",
    "                record_type = 'Brieven'\n",
    "            elif resource_model == 'Iconografie':\n",
    "                record_type = 'Prenten'\n",
    "            elif resource_model == 'Tekstdrager':\n",
    "                record_type = 'Handschriften'                    \n",
    "            \n",
    "            #print('if gr', resource_model, package, dataset, obj['brocade.subtype'][0]['value'], record_type)            \n",
    "        else:\n",
    "            record_type = lookup_column_value(obj['brocade.subtype'][0]['value'], \n",
    "                                            'record_types.csv',\n",
    "                                            card, obj['brocade.id'], '')\n",
    "            #print('not gr', resource_model, package, dataset, obj['brocade.subtype'][0]['value'], record_type)            \n",
    "            \n",
    "        obj_dict['Record Type'] = record_type \n",
    "        obj_list.append(obj_dict)\n",
    "        obj_dict = {}\n",
    "    source_df = pd.DataFrame(obj_list)\n",
    "#    source_df.to_csv('out/' + package + 'single.csv', index=False)\n",
    "\n",
    "    \n",
    "    save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "    return '%s saved: %s' % (card, len(source_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c0d04-7dec-434e-82c3-e10081577cfb",
   "metadata": {},
   "source": [
    "## Transaction History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b0488d3-24a3-4012-b2d2-51a9eb5a85a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '../../source/static/tg_history/'\n",
    "def make_transaction_history(resource_model, package, dataset):\n",
    "\n",
    "    read_name = source_dir  + dataset + '_transaction_history.csv'\n",
    "    save_name = '%s%s/%s/%s.csv' % (save_path, package, resource_model, 'e73_Transaction_history')\n",
    "    try:\n",
    "        shutil.copy2(read_name, save_name) \n",
    "        _return_msg = 'e73_Transaction_history.csv saved'\n",
    "    except:\n",
    "        _return_msg = ''\n",
    "    \n",
    "    return _return_msg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b9352-82a2-4fae-a0d8-0ffacfafa038",
   "metadata": {},
   "source": [
    "## Dams Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3191ef7c-a4d1-46fe-aa83-b6f3c1f260e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dams_dir = '../../source/static/dams/'\n",
    "def make_dams(resource_model, package, dataset):\n",
    "    read_name = dams_dir + 'e22_DamsLinks_' + package.split('_')[1] + '_' + resource_model + '.csv'\n",
    "    \n",
    "    if package in ['pkg_rub', 'pkg_mpm']:\n",
    "        save_name = '%s/%s/%s/%s.csv' % (save_path, package.split('_')[1], resource_model, 'e22_DamsLinks')\n",
    "    else:\n",
    "        save_name = '%s/%s/%s/%s.csv' % (save_path, 'lh', resource_model, 'e22_DamsLinks')\n",
    "    print(read_name)\n",
    "    print(save_name)\n",
    "    try:\n",
    "        shutil.copy2(read_name, save_name) \n",
    "        _return_msg = 'e22_DamsLinks saved'\n",
    "    except:\n",
    "        _return_msg = 'NO DAMS FILE FOUND'\n",
    "    \n",
    "    return _return_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f464ee1-f7af-494c-8d3e-a3108c9f80c3",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6549f0e3-1688-4a7b-863f-b5506d1d7c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_legacy_images(source_list, resource_model, package, dataset, card):\n",
    "\n",
    "    images_dir = '../../source/static/images/'\n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    images_in_resource_df = pd.DataFrame()\n",
    "    \n",
    "    for _dataset in resource_model_list[resource_model][package]:\n",
    "        try:\n",
    "            images_from_file_df = pd.read_csv(images_dir + _dataset + '_images.csv')\n",
    "            images_in_resource_df = images_in_resource_df.append(images_from_file_df, ignore_index=True)\n",
    "\n",
    "        except:\n",
    "            print('no such image file')\n",
    "        \n",
    "    source_df = pd.DataFrame(source_list)\n",
    "    resource_id_df = source_df['ResourceID']\n",
    "    result_df = pd.merge(images_in_resource_df, resource_id_df, on='ResourceID', how='inner')\n",
    "    result_df[' card'] = card\n",
    "    #result_df.to_csv('out/' + resource_model + '_result.csv', index=False)    \n",
    "\n",
    "    \n",
    "    if len(result_df) > 0:\n",
    "        save_it = save_mapped_set(result_df, package, resource_model, card)\n",
    "        number_of_records = len(result_df)\n",
    "        del source_df\n",
    "        del result_df\n",
    "        del images_from_file_df\n",
    "        del images_in_resource_df\n",
    "        return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd93b45-c09b-43c3-804d-dc36aa5a51d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Associated Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "733ec143-1662-4170-9b64-609f8f9c77e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_externals_df(source_df, column, args):\n",
    "    lookup_external_df = pd.read_csv(lookup_path + args['lookup_file'])\n",
    "    source_df[column] = source_df[column].map(lookup_external_df.set_index('code')['concept'])\n",
    "    return source_df \n",
    "\n",
    "def make_associated_records(source_list, resource_model, package, dataset, card):\n",
    "\n",
    "    source_df = pd.DataFrame(source_list)\n",
    "    mappings = multi_value_mappings[card]\n",
    "    \n",
    "    # make dataset\n",
    "    m_dict = {}\n",
    "    for column in source_df.columns:\n",
    "        for idx, row in source_df.iterrows():\n",
    "            if isinstance(row[column], list): \n",
    "                source_df.loc[idx, column] = row[column][0]['value']\n",
    "\n",
    "    source_df = source_df[source_df['ere.type'].isin(mappings['include_codes'])]    \n",
    "    # functions\n",
    "    for mapping in mappings['node_mappings']:\n",
    "        if 'function' in mapping.keys():            \n",
    "            source_df = lookup_externals_df(source_df, mapping['from'], mapping['function']['args'])\n",
    "        \n",
    "    # get ResourceID for the related object\n",
    "    \n",
    "    if package == \"pkg_rub\":\n",
    "        _dataset = 'rubgr'\n",
    "    else:\n",
    "        _dataset = dataset\n",
    "   \n",
    "    lookup_relations_df = pd.read_csv(lookup_e73_uiid_id_path + 'e73_' + _dataset + '_id_uuid_lookup.csv')\n",
    "    source_df['related_uuid'] = source_df['ere.url'].map(lookup_relations_df.set_index('brocade.id')['ResourceID'])\n",
    "    \n",
    "    for idx, row in source_df.iterrows():\n",
    "        source_df.loc[idx, 'import_relation'] = '[{\"resourceId\": \"%s\", \"ontologyProperty\": \"\", \"resourceXresourceId\": \"%s\", \"inverseOntologyProperty\": \"\"}]' % (row['related_uuid'], uuid.uuid4())\n",
    "\n",
    "\n",
    "    # do the mapping\n",
    "    for mapping in mappings['node_mappings']:\n",
    "        m_dict[mapping['from']] = mapping['to']\n",
    "    mapped_df = source_df.rename(columns=m_dict)    \n",
    "    #hack...\n",
    "    mapped_df = mapped_df.rename(columns={\"import_relation\": \"Associated Record\"})    \n",
    "    \n",
    "    # save\n",
    "    if len(mapped_df) > 0:\n",
    "        save_it = save_mapped_set(mapped_df, package, resource_model, card)\n",
    "\n",
    "    dim_dict = {}\n",
    "    dim_list = []\n",
    "    \n",
    "    return '%s saved: %s' % (card, len(mapped_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506540c2-0800-4bab-acb1-9edc907222db",
   "metadata": {},
   "source": [
    "## Places from Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57c92ec6-2a92-44e2-88af-950237ce564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_places_from_source_mpm(source_list, resource_model, package, dataset, card):\n",
    "    \n",
    "    creation_source_places_list = 'pkg_mpm' #[\"mpmbr\", \"mpmhs\", \"mpmph\", \"mpmtk\" ]\n",
    "    documentation_source_places_list = 'pkg_rub' #[\"rubhs\"]\n",
    "\n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "\n",
    "    \n",
    "    for obj in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in obj.keys():\n",
    "                obj_dict['ResourceID'] = obj['ResourceID']\n",
    "                obj_dict['brocade.id'] = obj['brocade.id']\n",
    "                obj_dict['card'] = card\n",
    "                obj_dict['order'] = obj['order']\n",
    "                obj_dict['source_field'] = node_mapping['from']\n",
    "                if obj[node_mapping['from']][0]['value'].startswith(\"au::\"):\n",
    "                    obj_dict['place_au_code'] = obj[node_mapping['from']][0]['value']                    \n",
    "                else:    \n",
    "                    obj_dict['place_note'] = obj[node_mapping['from']][0]['value']                    \n",
    "                    obj_dict['place_type'] = 'remarks'\n",
    "                    obj_dict['place_language'] = 'Nederlands'\n",
    "                \n",
    "                obj_list.append(obj_dict)\n",
    "                obj_dict = {}\n",
    "                \n",
    "    source_df = pd.DataFrame(obj_list)\n",
    "#    source_df.to_csv('out/' + resource_model + '_resu.csv', index=False)\n",
    "    \n",
    "    if len(obj_list) > 0:\n",
    "        lookup_place_json_df = pd.read_csv(lookup_path + 'aspace/as2arches_place.csv')\n",
    "        source_df['json'] = source_df['place_au_code'].map(lookup_place_json_df.set_index('brocadeID')['json'])\n",
    "        \n",
    "    if package in creation_source_places_list:\n",
    "        card = 'e73_CreationPlacesSource'\n",
    "        source_df = source_df.rename(columns={\"json\": \"Creation Place Widget\",\n",
    "                                             \"place_note\": \"Creation Place Note\",\n",
    "                                             \"place_type\": \"Creation Place Note Type\",\n",
    "                                             \"place_language\": \"Creation Place Note Language\"})\n",
    "\n",
    "    if package in documentation_source_places_list:\n",
    "        card = 'e73_DocumentedPlacesSource'\n",
    "        source_df = source_df.rename(columns={\"json\": \"Documented Place Name Widget\", \n",
    "                                                 \"order\": \"Documented Place Order\",\n",
    "                                                 \"place_note\": \"Documented Place Note\",\n",
    "                                                 \"place_type\": \"Documented Place Note Type\",\n",
    "                                                 \"place_language\": \"Documented Place Note Language\"})\n",
    "\n",
    "    \n",
    "    number_of_records = len(obj_list)\n",
    "    if number_of_records > 0:\n",
    "        save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "\n",
    "        del source_df\n",
    "        del obj_list\n",
    "        return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63cea7ac-4898-4198-b486-ff08f0aefd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_places_from_source_rub(source_list, resource_model, package, dataset, card):\n",
    "    \n",
    "    creation_source_places_list = 'pkg_mpm' #[\"mpmbr\", \"mpmhs\", \"mpmph\", \"mpmtk\" ]\n",
    "    documentation_source_places_list = ['tg:rubhs:88', 'tg:rubhs:108', 'tg:rubhs:145', 'tg:rubhs:137', 'tg:rubhs:111', \n",
    "                                        'tg:rubhs:91', 'tg:rubhs:81', 'tg:rubhs:75', 'tg:rubhs:65', 'tg:rubhs:110', \n",
    "                                        'tg:rubhs:80', 'tg:rubhs:64', 'tg:rubhs:74', 'tg:rubhs:89', 'tg:rubhs:109', \n",
    "                                        'tg:rubhs:144', 'tg:rubhs:136', 'tg:rubhs:92', 'tg:rubhs:82', 'tg:rubhs:59', \n",
    "                                        'tg:rubhs:112', 'tg:rubhs:76', 'tg:rubhs:66', 'tg:rubhs:134', 'tg:rubhs:146', \n",
    "                                        'tg:rubhs:135', 'tg:rubhs:147', 'tg:rubhs:83', 'tg:rubhs:113', 'tg:rubhs:67', \n",
    "                                        'tg:rubhs:77', 'tg:rubhs:72', 'tg:rubhs:62', 'tg:rubhs:139', 'tg:rubhs:174', \n",
    "                                        'tg:rubhs:86', 'tg:rubhs:116', 'tg:rubhs:106', 'tg:rubhs:131', 'tg:rubhs:143', \n",
    "                                        'tg:rubhs:153', 'tg:rubhs:55', 'tg:rubhs:138', 'tg:rubhs:63', 'tg:rubhs:73', \n",
    "                                        'tg:rubhs:175', 'tg:rubhs:87', 'tg:rubhs:107', 'tg:rubhs:141', 'tg:rubhs:133', \n",
    "                                        'tg:rubhs:68', 'tg:rubhs:78', 'tg:rubhs:57', 'tg:rubhs:71', 'tg:rubhs:61', \n",
    "                                        'tg:rubhs:158', 'tg:rubhs:115', 'tg:rubhs:105', 'tg:rubhs:85', 'tg:rubhs:60', \n",
    "                                        'tg:rubhs:70', 'tg:rubhs:84', 'tg:rubhs:94', 'tg:rubhs:176', 'tg:rubhs:79', \n",
    "                                        'tg:rubhs:69', 'tg:rubhs:132', 'tg:rubhs:56']\n",
    "\n",
    "\n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    obj_documented_list = []\n",
    "    obj_created_list = []\n",
    "\n",
    "    \n",
    "    for obj in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in obj.keys():\n",
    "                obj_dict['ResourceID'] = obj['ResourceID']\n",
    "                obj_dict['brocade.id'] = obj['brocade.id']\n",
    "                obj_dict['card'] = card\n",
    "                obj_dict['order'] = obj['order']\n",
    "                obj_dict['source_field'] = node_mapping['from']\n",
    "                if obj[node_mapping['from']][0]['value'].startswith(\"au::\"):\n",
    "                    obj_dict['place_au_code'] = obj[node_mapping['from']][0]['value']                    \n",
    "                else:    \n",
    "                    obj_dict['place_note'] = obj[node_mapping['from']][0]['value']                    \n",
    "                    obj_dict['place_type'] = 'remarks'\n",
    "                    obj_dict['place_language'] = 'Nederlands'\n",
    "                \n",
    "                if obj['brocade.id'] in documentation_source_places_list:\n",
    "                    obj_documented_list.append(obj_dict)\n",
    "                    obj_dict = {}\n",
    "                else:                        \n",
    "                    obj_created_list.append(obj_dict)\n",
    "                    obj_dict = {}\n",
    "                \n",
    "    source_documented_df = pd.DataFrame(obj_documented_list)\n",
    "    source_created_df = pd.DataFrame(obj_created_list)\n",
    "#    source_df.to_csv('out/' + resource_model + '_resu.csv', index=False)\n",
    "    \n",
    "    if len(obj_documented_list) > 0:\n",
    "        lookup_place_json_df = pd.read_csv(lookup_path + 'aspace/as2arches_place.csv')\n",
    "        source_documented_df['json'] = source_documented_df['place_au_code'].map(lookup_place_json_df.set_index('brocadeID')['json'])\n",
    "        card_documented = 'e73_DocumentedPlacesSource'\n",
    "        source_documented_df = source_documented_df.rename(columns={\"json\": \"Documented Place Name Widget\", \n",
    "                                                 \"order\": \"Documented Place Order\",\n",
    "                                                 \"place_note\": \"Documented Place Note\",\n",
    "                                                 \"place_type\": \"Documented Place Note Type\",\n",
    "                                                 \"place_language\": \"Documented Place Note Language\"})\n",
    "\n",
    "        number_of_doumented_records = len(obj_documented_list)\n",
    "        if number_of_doumented_records > 0:\n",
    "            save_it = save_mapped_set(source_documented_df, package, resource_model, card_documented)\n",
    "\n",
    "    if len(obj_created_list) > 0:\n",
    "        lookup_place_json_df = pd.read_csv(lookup_path + 'aspace/as2arches_place.csv')\n",
    "        source_created_df['json'] = source_created_df['place_au_code'].map(lookup_place_json_df.set_index('brocadeID')['json'])\n",
    "        card_created = 'e73_CreationPlacesSource'\n",
    "        source_created_df = source_created_df.rename(columns={\"json\": \"Creation Place Widget\",\n",
    "                                             \"place_note\": \"Creation Place Note\",\n",
    "                                             \"place_type\": \"Creation Place Note Type\",\n",
    "                                             \"place_language\": \"Creation Place Note Language\"})\n",
    "        \n",
    "        number_of_created_records = len(obj_created_list)\n",
    "        if number_of_created_records > 0:\n",
    "            save_it = save_mapped_set(source_created_df, package, resource_model, card_created)\n",
    "            \n",
    "\n",
    "        del source_documented_df\n",
    "        del source_created_df\n",
    "        obj_documented_list = []\n",
    "        obj_created_list = []\n",
    "        return '%s saved: %s, %s saved: %s' % (card_documented, number_of_doumented_records, card_created, number_of_created_records)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9e78f8-09aa-435f-b513-83b52c27547b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Places from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b993262-5939-4d6a-8fe3-67ad34d84d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_places_from_file(source_list, single_list, resource_model, package, dataset, card):\n",
    "\n",
    "    creation_file_places_list = [\"lhob\",  \"lhpr\", \"lhsc\", \"lhtk\", \"lhhs\", \"lhph\"]\n",
    "    documentation_file_places_list = [\"lhps\"]\n",
    "    \n",
    "    images_dir = '../../source/static/images/'\n",
    "    mappings = multi_value_mappings[card]\n",
    "    #columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    \n",
    "    \n",
    "    single_df = pd.DataFrame(single_list)\n",
    "    \n",
    "    try:\n",
    "        places_from_file_df = pd.read_csv(place_source_dir + dataset + '_places.csv')\n",
    "    except:\n",
    "        print('no such place file')\n",
    "                \n",
    "    if dataset in creation_file_places_list:\n",
    "        card = 'e73_CreationPlacesSource' + '_' + dataset\n",
    "        places_from_file_df['card'] = card\n",
    "        places_from_file_df = places_from_file_df.rename(columns={\"record_uuid\": \"ResourceID\",\n",
    "                                             \"brocade.id\": \"brocade.id\", \"json\": \"Creation Place Widget\",\n",
    "                                             \"note\": \"Creation Place Note\",\n",
    "                                             \"note_type\": \"Creation Place Note Type\",\n",
    "                                             \"note_language\": \"Creation Place Note Language\"})\n",
    "\n",
    "    if dataset in documentation_file_places_list:\n",
    "        card = 'e73_DocumentedEventsPlaces' + '_' + dataset\n",
    "        places_from_file_df['card'] = card\n",
    "        \n",
    "        places_from_file_df = places_from_file_df.rename(columns={\"record_uuid\": \"ResourceID\",\n",
    "                                              \"brocade.id\": \"brocade.id\",\"json\": \"Documented Event Place Name Widget\",                                                                   \n",
    "                                              \"order\": \"Documented Event Place Order\",\n",
    "                                              \"note\": \"Documented Event Place Note\",\n",
    "                                              \"note_type\": \"Documented Event Place Note Type\",\n",
    "                                              \"note_language\": \"Documented Event Place Note Language\"})\n",
    "\n",
    "    if len(places_from_file_df) > 0:\n",
    "        \n",
    "        # Hack to weed out extra places from the manually generated place file\n",
    "        #single_df = pd.DataFrame(single_list)\n",
    "        #filtered_df = places_from_file_df[places_from_file_df['ResourceID'].isin(single_df['ResourceID'])]\n",
    "        #save_it = save_mapped_set(filtered_df, package, resource_model, card)\n",
    "        #number_of_records = len(filtered_df)\n",
    "        #del places_from_file_df\n",
    "        #del filtered_df\n",
    "    \n",
    "        save_it = save_mapped_set(places_from_file_df, package, resource_model, card)\n",
    "        number_of_records = len(places_from_file_df)\n",
    "        del places_from_file_df\n",
    "        \n",
    "        return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a12b6-f291-4ee9-bc16-cb21ee1f1b2d",
   "metadata": {},
   "source": [
    "## Change History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a51d269-e29b-47a0-9add-4f5b4c2ac9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_change_history_relations(resource_model, package, card):\n",
    "\n",
    "    change_history_df = pd.read_csv('%s%s_Change History Log.csv' % (transactions_dir, package.split('_')[1])) \n",
    "    resource_model_df = change_history_df[change_history_df['resource_model']==resource_model]\n",
    "    relation_dict = {}\n",
    "    relation_list = []\n",
    "    related_resource_dict = {}\n",
    "    related_resource_list = []\n",
    "    \n",
    "\n",
    "    for idx, row in resource_model_df.iterrows():\n",
    "        relation_dict['ResourceID'] = row['source_uuid']\n",
    "        relation_dict['card'] = 'e73_ChangeHistoryRelations'\n",
    "        relation_dict['order'] = '1'\n",
    "        relation = '[{\"resourceId\": \"%s\", \"ontologyProperty\": \"\", \"resourceXresourceId\": \"%s\", \"inverseOntologyProperty\": \"\"}]' % (row['ResourceID'], str(uuid.uuid4()))\n",
    "        relation_dict['Transactions'] = relation\n",
    "\n",
    "        relation_list.append(relation_dict)    \n",
    "        relation_dict = {}\n",
    "        \n",
    "        related_resource_dict['resourceinstanceidfrom'] = row['source_uuid']\n",
    "        related_resource_dict['resourceinstanceidto'] = row['ResourceID']\n",
    "        related_resource_dict['relationshiptype'] = 'is related to'\n",
    "        related_resource_dict['datestarted'] = ''\n",
    "        related_resource_dict['dateended'] = ''\n",
    "        related_resource_dict['notes'] = ''\n",
    "                \n",
    "        related_resource_list.append(related_resource_dict)\n",
    "        related_resource_dict = {}        \n",
    "    \n",
    "    relation_df = pd.DataFrame(relation_list)\n",
    "    relation_df = relation_df.sort_values(by=['ResourceID'])\n",
    "    related_resource_df = pd.DataFrame(related_resource_list)\n",
    "    \n",
    "    save_it = save_mapped_set(relation_df, package, resource_model, card)\n",
    "    save_it = save_mapped_set(related_resource_df, package, resource_model, 'RelatedChangeHistory')\n",
    "\n",
    "\n",
    "    number_of_records = len(relation_df)\n",
    "    del change_history_df\n",
    "    del resource_model_df\n",
    "    del relation_df\n",
    "    del related_resource_list\n",
    "    del related_resource_df\n",
    "    return '%s saved: %s' % (card, number_of_records)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49af7a3-e878-4424-8527-343cd44e126d",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15b92b8c-4fb4-4c7b-ac65-8c40f8f7b7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foto\n",
      "-  pkg_rub\n",
      "   -  rubgr_foto\n",
      "      -  e73_AnnotationsImmaterial saved: 1\n",
      "      -  e73_AssociatedArchives saved: 25\n",
      "      -  e73_AssociatedRecords saved: 188\n",
      "      -  e73_CreationTimeSpanSourceNotes saved: 34\n",
      "      -  e73_Descriptions saved: 440\n",
      "      -  e73_ExternalRelationsExtras: found in source: 154, saved: 135\n",
      "      -  e73_ExternalRelations saved: 41\n",
      "      -  e73_Keywords saved: 449\n",
      "      -  e73_LegacyInformation saved: 1760\n",
      "      -  e73_MaterialRecords saved: 440\n",
      "      -  e73_Titles saved: 440\n",
      "      -  e73_Single saved: 440\n",
      "\n",
      "g_df = 0\n",
      "all_au_df = 331\n",
      "au_df (all_au_df - g_df) = 331\n",
      "e73_OriginalBrocade has NO Gs, all_au_df saved\n",
      "      -  e73_OriginalBrocade saved: 331\n",
      "      -  e73_ChangeHistoryRelations saved: 440\n",
      "../../source/static/dams/e22_DamsLinks_rub_Foto.csv\n",
      "../../../digipolis-arches-shoku-pkg/source/rub/Foto/e22_DamsLinks.csv\n",
      "      -  NO DAMS FILE FOUND\n",
      "Brief\n",
      "-  pkg_rub\n",
      "   -  rubgr_brief\n",
      "      -  e73_AnnotationsImmaterial saved: 6\n",
      "      -  e73_AssociatedArchives saved: 116\n",
      "      -  e73_AssociatedRecords saved: 683\n",
      "      -  e73_CreationTimeSpanSourceNotes saved: 1031\n",
      "      -  e73_Descriptions saved: 1132\n",
      "      -  e73_ExternalRelationsExtras: found in source: 399, saved: 310\n",
      "      -  e73_ExternalRelations saved: 48\n",
      "      -  e73_Keywords saved: 1429\n",
      "      -  e73_LegacyInformation saved: 4528\n",
      "      -  e73_MaterialRecords saved: 1132\n",
      "      -  e73_Titles saved: 1132\n",
      "      -  e73_Single saved: 1132\n",
      "\n",
      "g_df = 0\n",
      "all_au_df = 687\n",
      "au_df (all_au_df - g_df) = 687\n",
      "e73_OriginalBrocade has NO Gs, all_au_df saved\n",
      "      -  e73_OriginalBrocade saved: 687\n",
      "\n",
      "g_df = 0\n",
      "all_au_df = 1186\n",
      "au_df (all_au_df - g_df) = 1186\n",
      "e73_CreationActors has NO Gs, all_au_df saved\n",
      "      -  e73_CreationActors saved: 1186\n",
      "      -  e73_ChangeHistoryRelations saved: 1132\n",
      "../../source/static/dams/e22_DamsLinks_rub_Brief.csv\n",
      "../../../digipolis-arches-shoku-pkg/source/rub/Brief/e22_DamsLinks.csv\n",
      "      -  NO DAMS FILE FOUND\n",
      "Tekstdrager\n",
      "-  pkg_rub\n",
      "   -  rubgr_tekstdrager\n",
      "   -  rubdoc\n",
      "   -  rubhs\n",
      "      -  e73_DocumentedPlacesSource saved: 270, e73_CreationPlacesSource saved: 48\n",
      "      -  e73_AnnotationsImmaterial saved: 98\n",
      "      -  e73_AssociatedArchives saved: 537\n",
      "      -  e73_AssociatedRecords saved: 1865\n",
      "      -  e73_CreationTimeSpanSourceNotes saved: 4272\n",
      "      -  e73_Descriptions saved: 4371\n",
      "      -  e73_ExternalRelationsExtras: found in source: 1679, saved: 1615\n",
      "      -  e73_ExternalRelations saved: 1220\n",
      "      -  e73_Keywords saved: 5080\n",
      "      -  e73_Languages saved: 302\n",
      "      -  e73_LegacyInformation saved: 28189\n",
      "      -  e73_MaterialRecords saved: 8136\n",
      "      -  e73_Titles saved: 8136\n",
      "      -  e73_Single saved: 8136\n",
      "\n",
      "g_df = 0\n",
      "all_au_df = 4159\n",
      "au_df (all_au_df - g_df) = 4159\n",
      "e73_ConnectedSubjects has NO Gs, all_au_df saved\n",
      "      -  e73_ConnectedSubjects saved: 4159\n",
      "\n",
      "g_df = 0\n",
      "all_au_df = 2754\n",
      "au_df (all_au_df - g_df) = 2754\n",
      "e73_OriginalBrocade has NO Gs, all_au_df saved\n",
      "      -  e73_OriginalBrocade saved: 2754\n",
      "\n",
      "g_df = 0\n",
      "all_au_df = 4349\n",
      "au_df (all_au_df - g_df) = 4349\n",
      "e73_CreationActors has NO Gs, all_au_df saved\n",
      "      -  e73_CreationActors saved: 4349\n",
      "      -  e73_ChangeHistoryRelations saved: 8136\n",
      "../../source/static/dams/e22_DamsLinks_rub_Tekstdrager.csv\n",
      "../../../digipolis-arches-shoku-pkg/source/rub/Tekstdrager/e22_DamsLinks.csv\n",
      "      -  e22_DamsLinks saved\n",
      "Iconografie\n",
      "-  pkg_rub\n",
      "   -  rubgr_iconografie\n",
      "      -  e73_AssociatedArchives saved: 0\n",
      "      -  e73_Descriptions saved: 1\n",
      "      -  e73_ExternalRelationsExtras: found in source: 1, saved: 0\n",
      "      -  e73_Keywords saved: 3\n",
      "      -  e73_LegacyInformation saved: 4\n",
      "      -  e73_MaterialRecords saved: 1\n",
      "      -  e73_Titles saved: 1\n",
      "      -  e73_Single saved: 1\n",
      "      -  e73_ChangeHistoryRelations saved: 1\n",
      "../../source/static/dams/e22_DamsLinks_rub_Iconografie.csv\n",
      "../../../digipolis-arches-shoku-pkg/source/rub/Iconografie/e22_DamsLinks.csv\n",
      "      -  NO DAMS FILE FOUND\n",
      "---------\n",
      "2023-11-14 19:57:54.775711\n",
      "CPU times: user 12 s, sys: 593 ms, total: 12.5 s\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "file_places = [\"lhob\",  \"lhpr\", \"lhsc\", \"lhtk\", \"lhhs\", \"lhph\", \"lhps\"]\n",
    "source_places = [\"mpmbr\", \"mpmhs\", \"mpmph\", \"mpmtk\", \"rubhs\"]\n",
    "\n",
    "\n",
    "\n",
    "source_df = []\n",
    "record_dict = {}\n",
    "record_list = []\n",
    "records_list = []\n",
    "\n",
    "for resource_model, packages in resource_model_list.items():\n",
    "    print(resource_model)    \n",
    "    for package, datasets in packages.items():\n",
    "        \n",
    "        Languages_list = []\n",
    "        GenreTypes_List = []\n",
    "        Keywords_list = []\n",
    "        Correspondence_list = []\n",
    "        AssociatedArchives_list = []\n",
    "        ExternalRelationsExtras_list = []\n",
    "        Descriptions_list = []\n",
    "        CreationTimeSpanSourceNotes_list = []\n",
    "        Titles_list = [] \n",
    "        MaterialRecords_list = []\n",
    "        LegacyInformation_list = []\n",
    "        LegacyInformationMerged_list = []\n",
    "        LegacyImages_list = []\n",
    "            \n",
    "        # make_ire\n",
    "\n",
    "        CreationActors_list = []\n",
    "        RecordActors_list = []\n",
    "        \n",
    "        # make_exceptions\n",
    "        ConnectedSubjects_list = []\n",
    "        DepictedItems_list = []\n",
    "        OriginalBrocade_list = [] \n",
    "        \n",
    "        #from RDM\n",
    "        AnnotationsImmaterial_list = []\n",
    "        ExternalRelations_list = []\n",
    "        AssociatedRecords_list = []\n",
    "\n",
    "        Single_list = []\n",
    "        \n",
    "        PlacesSource_list = []\n",
    "        \n",
    "        \n",
    "        print('- ', package)\n",
    "        for dataset in datasets:\n",
    "            \n",
    "            if dataset_size == 'all':\n",
    "                source_file_name = source_path + dataset + '.json'\n",
    "            else:\n",
    "                source_file_name = source_path + 'slices/' + dataset + '_' + dataset_size +'.json'\n",
    "\n",
    "            with open(source_file_name) as f:\n",
    "                records = json.load(f)\n",
    "                \n",
    "            print('   - ', dataset)\n",
    "            for record in records:\n",
    "                if record['ID'].endswith('_#1'):                \n",
    "                    for card, v in multi_value_mappings.items():\n",
    "                        brocade_id = record['immaterialID']\n",
    "                        brocade_uuid = record['groups']['brocade']['Entries']['1']['Fields']['uuid'][0]['value']\n",
    "                        for group_key in get_group_keys(v): \n",
    "                            if (group_key in record['groups'].keys() or group_key == 'materialRecords'):\n",
    "\n",
    "                                if group_key == 'materialRecords':\n",
    "                                    record_dict['ResourceID'] = brocade_uuid\n",
    "                                    record_dict['brocade.id'] = brocade_id\n",
    "                                    record_dict['card'] = card\n",
    "                                    record_dict['order'] = order\n",
    "                                    record_dict['dataset'] = dataset\n",
    "                                    group_values = record[group_key]\n",
    "                                    record_dict[group_key] = record['materialRecords']\n",
    "                                    record_list.append(record_dict)\n",
    "                                    record_dict = {}   \n",
    "\n",
    "                                else:\n",
    "                                    group_values = record['groups'][group_key]\n",
    "                                    for order, item in group_values['Entries'].items():\n",
    "                                        if any(key in item['Fields'].keys() for key in get_field_keys(v)):\n",
    "                                            record_dict['ResourceID'] = brocade_uuid\n",
    "                                            record_dict['brocade.id'] = brocade_id\n",
    "                                            record_dict['card'] = card\n",
    "                                            record_dict['order'] = order\n",
    "                                            record_dict['dataset'] = dataset\n",
    "                                            for _key, _value in item['Fields'].items():\n",
    "                                                record_dict['%s.%s' % (group_key, _key)] = _value\n",
    "                                            record_list.append(record_dict)\n",
    "                                            record_dict = {}   \n",
    "                                \n",
    "                                if card == 'e73_AnnotationsImmaterial':\n",
    "                                    AnnotationsImmaterial_list.extend(record_list)\n",
    "                                if card == 'e73_AssociatedArchives': #uuid_immat OK\n",
    "                                    AssociatedArchives_list.extend(record_list)                                    \n",
    "                                if card == 'e73_AssociatedRecords':\n",
    "                                    AssociatedRecords_list.extend(record_list)\n",
    "                                if card == 'e73_ConnectedSubjects':\n",
    "                                    ConnectedSubjects_list.extend(record_list)                                    \n",
    "                                if card == 'e73_Correspondence': #uuid_immat OK \n",
    "                                    Correspondence_list.extend(record_list)                                    \n",
    "                                if card == 'e73_CreationActors':\n",
    "                                    CreationActors_list.extend(record_list)                                    \n",
    "                                if card == 'e73_CreationTimeSpanSourceNotes': #uuid_immat OK\n",
    "                                    CreationTimeSpanSourceNotes_list.extend(record_list)                                    \n",
    "                                if card == 'e73_DepictedItems':\n",
    "                                    DepictedItems_list.extend(record_list)                                    \n",
    "                                if card == 'e73_Descriptions': #uuid_immat OK\n",
    "                                    Descriptions_list.extend(record_list)                                    \n",
    "                                if card == 'e73_ExternalRelationsExtras': #uuid_immat OK\n",
    "                                    ExternalRelationsExtras_list.extend(record_list)  \n",
    "                                if card == 'e73_ExternalRelations':\n",
    "                                    ExternalRelations_list.extend(record_list)\n",
    "                                if card == 'e73_GenreTypes':\n",
    "                                    GenreTypes_List.extend(record_list)                                    \n",
    "                                if card == 'e73_Keywords': #uuid_immat OK\n",
    "                                    Keywords_list.extend(record_list)                                    \n",
    "                                if card == 'e73_Languages':\n",
    "                                    Languages_list.extend(record_list)                                    \n",
    "                                if card == 'e73_LegacyInformation':\n",
    "                                    LegacyInformation_list.extend(record_list)                                    \n",
    "                                if card == 'e73_LegacyInformationMerged':\n",
    "                                    LegacyInformationMerged_list.extend(record_list)                                    \n",
    "                                    \n",
    "\n",
    "                                if card == 'e73_Titles': #uuid_immat OK\n",
    "                                    Titles_list.extend(record_list)                                    \n",
    "                                if card == 'e73_MaterialRecords': #uuid_immat OK\n",
    "                                    MaterialRecords_list.extend(record_list)                                    \n",
    "                                    \n",
    "                                if card == 'e73_RecordActors':\n",
    "                                    RecordActors_list.extend(record_list)                                    \n",
    "                                if card == 'e73_OriginalBrocade':\n",
    "                                    OriginalBrocade_list.extend(record_list)                                    \n",
    "                                if card == 'e22_ProductionPlacesSource':\n",
    "                                    ProductionPlacesSource_list.extend(record_list)                                    \n",
    "                                    \n",
    "                                    \n",
    "\n",
    "                                if card == 'e73_Single':\n",
    "                                    Single_list.extend(record_list)                                    \n",
    "                                if card == 'Places':\n",
    "                                    PlacesSource_list.extend(record_list)                                    \n",
    "                                    \n",
    "                                    \n",
    "                                record_list = []                                \n",
    "\n",
    "\n",
    "            if dataset in file_places:\n",
    "                if len(PlacesSource_list) > 0:                \n",
    "                    PlacesFromFiles = make_places_from_file(PlacesSource_list, Single_list, resource_model, package, dataset, 'Places')\n",
    "                    print('      - ', PlacesFromFiles)\n",
    "                   \n",
    "        if package in ['pkg_mpm']:\n",
    "            if len(PlacesSource_list) > 0:\n",
    "                PlacesSource = make_places_from_source_mpm(PlacesSource_list, resource_model, package, dataset, 'Places')\n",
    "                print('      - ', PlacesSource)                \n",
    "        if package in ['pkg_rub']:\n",
    "            if len(PlacesSource_list) > 0:\n",
    "                PlacesSource = make_places_from_source_rub(PlacesSource_list, resource_model, package, dataset, 'Places')\n",
    "                print('      - ', PlacesSource)                \n",
    "\n",
    "                \n",
    "        if len(AnnotationsImmaterial_list) > 0:\n",
    "            AnnotationsImmaterial = make_annotations(AnnotationsImmaterial_list, resource_model, package, dataset, 'e73_AnnotationsImmaterial', 'e73')\n",
    "            print('      - ', AnnotationsImmaterial)    \n",
    "        if len(AssociatedArchives_list) > 0:\n",
    "            AssociatedArchives = make_associated_archive(AssociatedArchives_list, resource_model, package, dataset, 'e73_AssociatedArchives')\n",
    "            print('      - ', AssociatedArchives)    \n",
    "        if len(AssociatedRecords_list) > 0:\n",
    "            AssociatedRecords = make_associated_records(AssociatedRecords_list, resource_model, package, dataset, 'e73_AssociatedRecords')\n",
    "            print('      - ', AssociatedRecords)    \n",
    "        if len(Correspondence_list) > 0:\n",
    "            Correspondence = make_correspondence(Correspondence_list, resource_model, package, dataset, 'e73_Correspondence')\n",
    "        if len(CreationTimeSpanSourceNotes_list) > 0:\n",
    "            CreationTimeSpanSourceNotes = make_plain_mappings(CreationTimeSpanSourceNotes_list, resource_model, package, dataset, 'e73_CreationTimeSpanSourceNotes')\n",
    "            print('      - ', CreationTimeSpanSourceNotes)\n",
    "        if len(Descriptions_list) > 0:\n",
    "            Descriptions = make_descriptions(Descriptions_list, resource_model, package, dataset, 'e73_Descriptions')\n",
    "            print('      - ', Descriptions)    \n",
    "        if len(ExternalRelationsExtras_list) > 0:\n",
    "            ExternalRelationsExtras = make_external_relations_extra(ExternalRelationsExtras_list, resource_model, package, dataset, 'e73_ExternalRelationsExtras')\n",
    "            print('      - ', ExternalRelationsExtras)                            \n",
    "        if len(ExternalRelations_list) > 0:\n",
    "            ExternalRelations = make_external_relations(ExternalRelations_list, resource_model, package, dataset, 'e73_ExternalRelations')\n",
    "            print('      - ', ExternalRelations)    \n",
    "        if len(GenreTypes_List) > 0:\n",
    "            GenreTypes = e73_make_with_function_and_include_codes(GenreTypes_List, resource_model, package, dataset, 'e73_GenreTypes')\n",
    "            print('      - ', GenreTypes)    \n",
    "        if len(Keywords_list) > 0:\n",
    "            Keywords = make_plain_mappings(Keywords_list, resource_model, package, dataset, 'e73_Keywords')\n",
    "            print('      - ', Keywords)    \n",
    "        if len(Languages_list) > 0:\n",
    "            Languages = e73_make_with_function_and_include_codes(Languages_list, resource_model, package, dataset, 'e73_Languages')\n",
    "            print('      - ', Languages)    \n",
    "        if len(LegacyInformation_list) > 0:\n",
    "            LegacyInformation = make_legacy_info(LegacyInformation_list, resource_model, package, dataset, 'e73_LegacyInformation')\n",
    "            LegacyInformation_list = []\n",
    "            print('      - ', LegacyInformation)    \n",
    "\n",
    "# Check if it can be made faster...\n",
    "        if len(LegacyInformationMerged_list) > 0:\n",
    "            LegacyInformationMerged = make_legacy_info_merged(LegacyInformationMerged_list, resource_model, package, dataset, 'e73_LegacyInformationMerged')\n",
    "            LegacyInformationMerged_list = []\n",
    "            print('      - ', LegacyInformationMerged)    \n",
    "\n",
    "        if len(MaterialRecords_list) > 0:\n",
    "            MaterialRecords = make_material_records(MaterialRecords_list, resource_model, package, dataset, 'e73_MaterialRecords')\n",
    "            MaterialRecords_list = []\n",
    "            print('      - ', MaterialRecords)    \n",
    "        if len(Titles_list) > 0:\n",
    "            Titles = make_titles(Titles_list, resource_model, package, dataset, 'e73_Titles')\n",
    "            Titles_list = []\n",
    "            print('      - ', Titles)                                  \n",
    "        if len(Single_list) > 0:\n",
    "            Single = make_single(Single_list, resource_model, package, dataset, 'e73_Single')\n",
    "            print('      - ', Single)                                                          \n",
    "        if len(Single_list) > 0:\n",
    "            if package.startswith(\"pkg_lh\"):                \n",
    "                LegacyImages = make_legacy_images(Single_list, resource_model, package, dataset, 'e73_LegacyImages')\n",
    "                Single_list = []\n",
    "                print('      - ', LegacyImages)   \n",
    "                   \n",
    "        if len(ConnectedSubjects_list) > 0:\n",
    "            ConnectedSubjects = make_exception_ire(ConnectedSubjects_list, resource_model, package, dataset, 'e73_ConnectedSubjects', 'e73')\n",
    "            if not ConnectedSubjects is None:\n",
    "                print('      - ', ConnectedSubjects)    \n",
    "        if len(DepictedItems_list) > 0:\n",
    "            DepictedItems = make_exception_ire(DepictedItems_list, resource_model, package, dataset, 'e73_DepictedItems', 'e73')\n",
    "            if not DepictedItems is None:\n",
    "                print('      - ', DepictedItems)    \n",
    "        if len(OriginalBrocade_list) > 0:\n",
    "            OriginalBrocade = make_exception_ire(OriginalBrocade_list, resource_model, package, dataset, 'e73_OriginalBrocade', 'e73')\n",
    "            if not OriginalBrocade is None:\n",
    "                print('      - ', OriginalBrocade)    \n",
    "        if len(CreationActors_list) > 0:\n",
    "            CreationActors = make_plain_ire(CreationActors_list, resource_model, package, dataset, 'e73_CreationActors', 'e73')\n",
    "            if not CreationActors is None:\n",
    "                print('      - ', CreationActors)    \n",
    "        if len(RecordActors_list) > 0:\n",
    "            RecordActors = make_plain_ire(RecordActors_list, resource_model, package, dataset, 'e73_RecordActors', 'e73')\n",
    "            RecordActors_list = []\n",
    "            if not RecordActors is None:\n",
    "                print('      - ', RecordActors)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ChangeHistory = make_change_history_relations(resource_model, package, 'e73_ChangeHistoryRelations')\n",
    "    print('      - ', ChangeHistory)   \n",
    "\n",
    "    DamsLinks = make_dams(resource_model, package, dataset)\n",
    "    print('      - ', DamsLinks)  \n",
    "\n",
    "                   \n",
    "print('---------') \n",
    "print(datetime.datetime.now())\n",
    "%reset -f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a4ec6-7d00-443c-a037-adfc7ab5ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c230a-1b05-40e6-98fc-f206c6bf324a",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c778533-c7df-4bcb-80ab-15b622290234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
