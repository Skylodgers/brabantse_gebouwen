{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0ed3d9-970b-4cd7-b1c2-052e114505e3",
   "metadata": {},
   "source": [
    "## Cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dbf0952-c2ca-4665-b3f1-840969562e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from edtf import text_to_edtf\n",
    "import datetime\n",
    "from import_spec import * # package and number of records to import, see import_spec.py\n",
    "\n",
    "\n",
    "lookup_path = '../../lookup/'\n",
    "source_path = '../../source/tg/'\n",
    "mappings_path = '../../mappings/'\n",
    "save_path = '../../../digipolis-arches-shoku-pkg/source/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9828891-f286-4f4e-a6b5-10c434f537bd",
   "metadata": {},
   "source": [
    "## Lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0ed5d68-0e8e-40f3-8660-f17d097b3302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes\n",
    "lookup_acquisitions_df = pd.read_csv(lookup_path + 'aspace/as2arches_accessions.csv')\n",
    "lookup_archives_df = pd.read_csv(lookup_path + 'aspace/as2arches_archiveobject.csv')\n",
    "lookup_imaginary_df = pd.read_csv(lookup_path + 'aspace/as2arches_merged.csv')\n",
    "lookup_relation_df = pd.read_csv(lookup_path + 'isaar_relatie_types.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0688898-a733-4a4f-b2bb-c642cdfe2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(mappings_path + 'mappings_e22.json') as f:\n",
    "    multi_value_mappings = json.load(f)  \n",
    "\n",
    "with open(mappings_path + 'resource_models_%s.json' % (resource_model) ) as f:\n",
    "    resource_model_list = json.load(f)   \n",
    "\n",
    "with open(mappings_path + 'static.json') as f:\n",
    "    static_list = json.load(f)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af069a79-928f-4753-9f61-9b269a13f3c6",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "220cfa93-c1df-499b-a1bc-0292f69f597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ark_identifier_by_row_value(package, uuid):\n",
    "    return '%s/ark:/%s/%s' % (static_list[package]['ark_url'], static_list[package]['naan'], uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f41b1fef-ee8b-4636-a0e0-8b430008c8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ead_identifier_by_row_value(package, uuid):\n",
    "    return '%s%s' % (static_list[package]['ead_uri'], uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33518a92-bf2b-4740-a94d-b81e9e8cba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ark_identifier_df(source_df, uuid_column, package):\n",
    "    for idx, row in source_df.iterrows():\n",
    "        source_df.loc[idx, _args['column']] = '%s/ark:/%s/%s'  % (static_list[package]['ark_url'], static_list[package]['naan'], row[uuid_column])         \n",
    "    return source_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34071b93-9fd3-48a6-b04c-f9a1f75c5532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_column_df(source_df, column, args):\n",
    "    lookup_df = pd.read_csv(lookup_path + args['lookup_file'])\n",
    "    source_df[column] = source_df[args['match_source']].map(lookup_df.set_index(args['code'])[args['concept']])\n",
    "    del lookup_df\n",
    "    return source_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3247280-3013-46a5-95f2-aaba0fd77127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_column_value(code, lookup_file, card, brocade_id, source_field):\n",
    "    lookup_df = pd.read_csv(lookup_path + lookup_file)\n",
    "    try:\n",
    "        concept_name = lookup_df[lookup_df['code'] == code]['concept'].iloc[0]\n",
    "        del lookup_df\n",
    "        return concept_name\n",
    "    except:\n",
    "        #print('%s,%s,%s,%s,%s' % (card, brocade_id, source_field, code, lookup_file))\n",
    "        print(brocade_id, ',', code + ',NOT FOUND IN,' + lookup_file)\n",
    "        return code + ' NOT FOUND IN: ' + lookup_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63128f99-6fd0-4362-b5b8-69f27a600fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mapped_set(source_df, _package, _resource_model, _card):\n",
    "    \n",
    "    if _package in ['pkg_lhps', 'pkg_lhph', 'pkg_lhbr']:\n",
    "        package = 'lh'\n",
    "    else:\n",
    "        package = _package.split('_')[1]     \n",
    "    \n",
    "    file_name = '%s/%s/%s/%s.csv' % (save_path, package, _resource_model, _card)\n",
    "    source_df.to_csv(file_name, index=False)    \n",
    "    del source_df\n",
    "    return 'Saved OK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff01ed37-2072-4cc2-92f6-6d38f444130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constant_type(mapping, source_key):\n",
    "    # pull constants from a node mapping\n",
    "    for node_mapping in mapping['node_mappings']:\n",
    "        if node_mapping['from'] == source_key:\n",
    "            return node_mapping['constants']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f91b4600-d80c-4151-a0ec-9dcb4d380f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_default_dict(obj, card, source_value, source_field, model_class):\n",
    "\n",
    "    default_dict = {}\n",
    "    if model_class == 'e73':\n",
    "        default_dict['ResourceID'] = obj['ResourceID_immat']\n",
    "    elif model_class == 'e22':   \n",
    "        default_dict['ResourceID'] = obj['ResourceID']\n",
    "    default_dict['brocade.id'] = obj['brocade.id']\n",
    "    default_dict['card'] = card\n",
    "    default_dict['order'] = obj['order']\n",
    "    default_dict['source_field'] = source_field\n",
    "    default_dict['source_code'] =  source_value\n",
    "    \n",
    "    return default_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b865f646-1fd5-4740-9fc8-d5c638dc6997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mark_type(mapping, source_key):\n",
    "\n",
    "    for node_mapping in mapping['node_mappings']:\n",
    "        if node_mapping['from'] == source_key:\n",
    "            return node_mapping['constants']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0173431-3758-49c9-be28-d0e612c2bfa1",
   "metadata": {},
   "source": [
    "## Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fe688e1-e408-4ec4-b08b-dd36b1cdaa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_keys(v):\n",
    "    group_key_list = []\n",
    "    for mapping in v['node_mappings']:\n",
    "        group_key_list.append(mapping['from'].split('.')[0])\n",
    "    group_key_list = list(dict.fromkeys(group_key_list) ) \n",
    "    return(group_key_list)\n",
    "\n",
    "def get_field_keys(v):\n",
    "    field_list = []\n",
    "    try:\n",
    "        for mapping in v['node_mappings']:\n",
    "            field_list.append(mapping['from'].split('.')[1])\n",
    "        return(field_list)\n",
    "    except:\n",
    "        print('Error in:', mapping)\n",
    "        \n",
    "def get_from_keys(v):\n",
    "    map_list = []\n",
    "    for mapping in v['node_mappings']:\n",
    "        map_list.append(mapping['from'])\n",
    "    return(map_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecbd81c-2a6f-484f-a7b2-e11455c14826",
   "metadata": {},
   "source": [
    "## Cards:\n",
    "\n",
    "* Genre Types \n",
    "* Languages \n",
    "* Merkteken Types \n",
    "* Writing Material\n",
    "* ProductStadiums\n",
    "* Document status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f99bae3-cc11-4ae6-a290-6c4504cafbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e22_make_with_function_and_include_codes(source_list, resource_model, package, dataset, card):\n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    default_dict_items = {}\n",
    "    \n",
    "    for obj in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in obj.keys():\n",
    "                source_value = obj[node_mapping['from']][0]['value']\n",
    "                if 'include_codes' in node_mapping.keys(): \n",
    "                    if source_value in node_mapping['include_codes']:\n",
    "                        obj_dict.update(make_default_dict(obj, card, source_value, node_mapping['from'], 'e22'))\n",
    "                        if 'function' in node_mapping.keys():\n",
    "                            if node_mapping['function']['name'] == 'column_lookup': \n",
    "                                _type = lookup_column_value(obj[node_mapping['from']][0]['value'], \n",
    "                                            node_mapping['function']['args']['lookup_file'],\n",
    "                                            card, obj['brocade.id'], node_mapping['from'])\n",
    "                                obj_dict[node_mapping['to']] = _type                            \n",
    "                        obj_list.append(obj_dict)\n",
    "                        obj_dict = {}\n",
    "\n",
    "    source_df = pd.DataFrame(obj_list)\n",
    "    number_of_records = len(obj_list)\n",
    "    save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "    del source_df\n",
    "    del source_list\n",
    "    del obj\n",
    "    return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c5775-b2a3-4a67-8d42-dcd246c72f21",
   "metadata": {},
   "source": [
    "## Carriers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a571e94a-70b2-4610-bdf7-29698ffba125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_carriers(source_list, resource_model, package, dataset, card):\n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    default_dict_items = {}\n",
    "    \n",
    "    for obj in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in obj.keys():\n",
    "                source_value = obj[node_mapping['from']][0]['value']\n",
    "                if 'include_codes' in node_mapping.keys(): \n",
    "                    if source_value in node_mapping['include_codes']:\n",
    "                        if 'function' in node_mapping.keys():\n",
    "                            if node_mapping['function']['name'] == 'column_lookup': \n",
    "                                _type = lookup_column_value(obj[node_mapping['from']][0]['value'], \n",
    "                                            node_mapping['function']['args']['lookup_file'],\n",
    "                                            card, obj['brocade.id'], node_mapping['from'])\n",
    "                                obj_dict[node_mapping['to']] = _type\n",
    "                        else:\n",
    "                            obj_dict[node_mapping['to']] = source_value\n",
    "\n",
    "                if node_mapping['to'] == 'Carrier Type': \n",
    "                    obj_dict['carrier_type_source_field'] = node_mapping['from']\n",
    "                    \n",
    "                if node_mapping['to'] == 'Carrier Colour': \n",
    "                    obj_dict['carrier_colour_source_field'] = node_mapping['from']                \n",
    "                \n",
    "        if len(obj_dict) > 0:\n",
    "            obj_dict.update(make_default_dict(obj, card, '', '', 'e22'))\n",
    "        \n",
    "        obj_list.append(obj_dict)\n",
    "        obj_dict = {}\n",
    "\n",
    "    source_df = pd.DataFrame(obj_list)\n",
    "    number_of_records = len(obj_list)\n",
    "    if 'Carrier Type' in source_df.columns and 'Carrier Colour' in source_df.columns:\n",
    "        source_df = source_df[['ResourceID', 'brocade.id', 'card', 'order', 'carrier_type_source_field', 'Carrier Type', 'carrier_colour_source_field', 'Carrier Colour']]\n",
    "    elif 'Carrier Type' in source_df.columns:\n",
    "        source_df = source_df[['ResourceID', 'brocade.id', 'card', 'order', 'carrier_type_source_field', 'Carrier Type']]\n",
    "    else:\n",
    "        source_df = source_df[['ResourceID', 'brocade.id', 'card', 'order', 'carrier_colour_source_field', 'Carrier Colour']]\n",
    "        \n",
    "    #source_df.to_csv('out/' + package.split('_')[1] + '_' + resource_model + '_carrier.csv', index=False)\n",
    "    \n",
    "    save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "    del source_df\n",
    "    del source_list\n",
    "    del obj\n",
    "    return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff64b84c-0083-4420-bc43-c8f11731e4e2",
   "metadata": {},
   "source": [
    "## Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5248909c-b413-41c0-9f41-0073003b5590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mark_type(mapping, source_key):\n",
    "\n",
    "    for node_mapping in mapping['node_mappings']:\n",
    "        if node_mapping['from'] == source_key:\n",
    "            return node_mapping['constants']\n",
    "    \n",
    "def make_marks(source_list, resource_model, package, dataset, card):\n",
    "\n",
    "    dim_dict = {}\n",
    "    dim_list = []\n",
    "    card = card\n",
    "    mapping = multi_value_mappings[card]\n",
    "    \n",
    "    for dim in source_list:\n",
    "        for node_mapping in mapping['node_mappings']:\n",
    "            if node_mapping['from'] in dim.keys():\n",
    "                dim_dict['ResourceID'] = dim['ResourceID']\n",
    "                dim_dict['brocade.id'] = dim['brocade.id']\n",
    "                dim_dict['card'] = card\n",
    "                dim_dict['order'] = dim['order']\n",
    "                dim_dict[node_mapping['to']] = dim[node_mapping['from']][0]['value']\n",
    "                constants = get_mark_type(mapping, node_mapping['from'])\n",
    "                for _type, _value in constants.items():\n",
    "                    dim_dict[_type] = _value\n",
    "                dim_list.append(dim_dict)\n",
    "                dim_dict = {}\n",
    "\n",
    "    source_df = pd.DataFrame(dim_list)\n",
    "    number_of_records = len(dim_list)    \n",
    "    # save\n",
    "    save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "    #source_df.to_csv('%s_%s_Group_%s.csv' % (resource_model, card, package), index=False)\n",
    "\n",
    "    dim_dict = {}\n",
    "    dim_list = []\n",
    "    del source_df\n",
    "    del source_list\n",
    "\n",
    "    return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47190c42-bb24-4543-a7d7-3013ad9a0966",
   "metadata": {},
   "source": [
    "## Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "658ec45c-62e2-43fc-a329-d5d48953fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_items(source_list, resource_model, package, dataset, card):\n",
    "    usr_dict = {}\n",
    "    usr_list = []\n",
    "    mappings = multi_value_mappings[card]\n",
    "\n",
    "    source_df = pd.DataFrame(source_list)\n",
    "    # make dataset\n",
    "    m_dict = {}\n",
    "    for column in source_df.columns:\n",
    "        for idx, row in source_df.iterrows():\n",
    "            if isinstance(row[column], list): \n",
    "                source_df.loc[idx, column] = row[column][0]['value']\n",
    "            if column == 'adm.note':\n",
    "                if pd.notna(row['adm.note']):\n",
    "                    source_df.loc[idx, 'Source Note Type'] = 'remarks'\n",
    "                    source_df.loc[idx, 'Source Note Language'] = 'Nederlands'\n",
    "\n",
    "    if package in ['pkg_lh', 'pkg_lhps', 'pkg_lhbr', 'pkg_lhph']:\n",
    "        source_df = source_df.drop(columns=['adm.note', 'Source Note Type', 'Source Note Language'])\n",
    "    if package in ['pkg_rub']:\n",
    "        source_df = source_df.drop(columns=['adm.volgnummer'])\n",
    "    if package in ['pkg_mpm']:\n",
    "        source_df = source_df.rename(columns={'adm.inv': 'Plaatsingsnummer'})\n",
    "\n",
    "    \n",
    "    # do the mapping\n",
    "    for node_mappings in mappings['node_mappings']:\n",
    "        usr_dict[node_mappings['from']] = node_mappings['to']        \n",
    "    dataset_df = source_df.rename(columns=usr_dict)\n",
    "    dataset_df['card'] = card\n",
    "    #dataset_df.to_csv('out/' + resource_model + '_item.csv', index=False)\n",
    "\n",
    "    # save\n",
    "    save_it = save_mapped_set(dataset_df, package, resource_model, card)\n",
    "    \n",
    "    number_of_records = len(dataset_df)\n",
    "    m_dict = {}\n",
    "    usr_dict = []\n",
    "    del source_df\n",
    "    del dataset_df\n",
    "    del source_list\n",
    "    \n",
    "    return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce700f09-a605-4823-844e-5ce23f488a55",
   "metadata": {},
   "source": [
    "## Associated Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cd2213e-6802-4ca7-8cda-f09d5c7ee0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_aspace_archives_df = pd.read_csv(lookup_path + 'aspace/as2arches_archiveobject.csv')\n",
    "lookup_rub_isad_df = pd.read_csv(lookup_path + 'rub_isad_internal_relations.csv')\n",
    "\n",
    "def make_associated_archive(source_list, resource_model, package, dataset, card):\n",
    "    \n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    \n",
    "    for obj in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in obj.keys():\n",
    "                if obj['order'] == '2':\n",
    "                    obj_dict['ResourceID'] = obj['ResourceID']\n",
    "                    obj_dict['brocade.id'] = obj['brocade.id']\n",
    "                    obj_dict['card'] = card\n",
    "                    obj_dict['order'] = obj['order']\n",
    "                    obj_dict[node_mapping['from']] = obj[node_mapping['from']][0]['value']                \n",
    "                    obj_list.append(obj_dict)\n",
    "                    obj_dict = {}\n",
    "\n",
    "    source_df = pd.DataFrame(obj_list)\n",
    "    number_of_records = len(obj_list)    \n",
    "    \n",
    "    if (len(source_df) > 0):\n",
    "        source_df['isaad'] = source_df[node_mapping['from']].map(lookup_rub_isad_df.set_index('code')['isad'])\n",
    "        source_df['Associated Archive'] = source_df['isaad'].map(lookup_aspace_archives_df.set_index('brocadeID')['json'])\n",
    "\n",
    "    #source_df.to_csv('out/' + dataset + '_ass_arch.csv')\n",
    "\n",
    "#    for node_mappings in mappings['node_mappings']:\n",
    "#        obj_dict[node_mappings['from']] = node_mappings['to']        \n",
    "#    dataset_df = source_df.rename(columns=obj_dict)\n",
    "    dataset_df = source_df\n",
    "    if len(dataset_df) > 0:\n",
    "        save_it = save_mapped_set(dataset_df, package, resource_model, card)\n",
    "    \n",
    "    del source_df\n",
    "    del dataset_df\n",
    "    del source_list\n",
    "    del obj_list\n",
    "    return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea9af60-65f1-4ee8-b306-18f56762f9cd",
   "metadata": {},
   "source": [
    "# Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94ba63df-ce3a-4b8b-a444-18a8055edb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_format_type(mapping, source_key):\n",
    "\n",
    "    for node_mapping in mapping['node_mappings']:\n",
    "        if node_mapping['from'] == source_key:\n",
    "            return node_mapping['constants']\n",
    "        \n",
    "def make_formats(source_list, resource_model, package, dataset, card):\n",
    "    \n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    \n",
    "    for obj in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in obj.keys():\n",
    "                obj_dict['ResourceID'] = obj['ResourceID']\n",
    "                obj_dict['brocade.id'] = obj['brocade.id']\n",
    "                obj_dict['card'] = card\n",
    "                obj_dict['order'] = obj['order']\n",
    "                obj_dict['source_field'] = node_mapping['from']\n",
    "                obj_dict[node_mapping['to']] = obj[node_mapping['from']][0]['value']\n",
    "                if 'constants' in node_mapping.keys():\n",
    "                    constants = get_format_type(mappings, node_mapping['from'])\n",
    "                    for _const, _value in constants.items():\n",
    "                        obj_dict[_const] = _value\n",
    "                if 'function' in node_mapping.keys():\n",
    "                    if node_mapping['function']['args']['from'] in obj.keys():\n",
    "                        obj_dict[node_mapping['function']['args']['to']] = obj[node_mapping['function']['args']['from']][0]['value']\n",
    "\n",
    "\n",
    "                obj_list.append(obj_dict)\n",
    "                obj_dict = {}\n",
    "    source_df = pd.DataFrame(obj_list)\n",
    "    number_of_records = len(obj_list)    \n",
    "    \n",
    "    save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "    \n",
    "    del source_df\n",
    "    del source_list\n",
    "    del obj_list\n",
    "    return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b951fb-0221-4f47-a361-c0b30c20b4d7",
   "metadata": {},
   "source": [
    "## Cards:\n",
    "* Material \n",
    "* Colour\n",
    "* Production Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b16f848c-aac1-4c10-8718-84c243ef1108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_with_include_codes(source_list, resource_model, package, dataset, card, model_class):\n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    default_dict_items = {}\n",
    "    \n",
    "    for obj in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in obj.keys():\n",
    "                source_value = obj[node_mapping['from']][0]['value']\n",
    "                if 'include_codes' in node_mapping.keys(): \n",
    "                    if source_value in node_mapping['include_codes']:\n",
    "                        obj_dict.update(make_default_dict(obj, card, source_value, node_mapping['from'], model_class))\n",
    "                        if 'function' in node_mapping.keys():\n",
    "                            if node_mapping['function']['name'] == 'column_lookup': \n",
    "                                _type = lookup_column_value(obj[node_mapping['from']][0]['value'], \n",
    "                                            node_mapping['function']['args']['lookup_file'],\n",
    "                                            card, obj['brocade.id'], node_mapping['from'])\n",
    "                                obj_dict[node_mapping['to']] = _type                            \n",
    "                        obj_list.append(obj_dict)\n",
    "                        obj_dict = {}\n",
    "\n",
    "    source_df = pd.DataFrame(obj_list)\n",
    "    \n",
    "    if package == 'pkg_rub' and card == 'e22_ProductionTechniques' and resource_model == 'Foto':\n",
    "        drop_codes_list = [\"dtrvnt\"]        \n",
    "        source_df = source_df.loc[source_df['source_code'].isin(drop_codes_list) == False]\n",
    "        drop_codes_list = []\n",
    "    \n",
    "    number_of_records = len(source_df)\n",
    "    save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "\n",
    "    del source_df\n",
    "    del source_list\n",
    "    del obj_list\n",
    "    return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e832210-cb9e-4fae-a1dc-788dc0deff5b",
   "metadata": {},
   "source": [
    "## Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff48b444-6661-421b-9345-db259308dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_archives(source_list, resource_model, package, dataset, card):\n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    \n",
    "    for obj in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in obj.keys():\n",
    "                #obj_dict['ResourceID'] = obj['adm.uuid_mat'][0]['value']\n",
    "                #obj_dict['brocade.id'] = obj['adm.id_mat'][0]['value']\n",
    "                obj_dict['ResourceID'] = obj['ResourceID']\n",
    "                obj_dict['brocade.id'] = obj['brocade.id']\n",
    "                obj_dict['card'] = card\n",
    "                obj_dict['order'] = obj['order']\n",
    "                obj_dict[node_mapping['to']] = obj[node_mapping['from']][0]['value']\n",
    "                obj_list.append(obj_dict)\n",
    "                obj_dict = {}\n",
    "    source_df = pd.DataFrame(obj_list)\n",
    "    number_of_records = len(obj_list)\n",
    "    source_df['Archive Widget'] = source_df[node_mapping['to']].map(lookup_archives_df.set_index('brocadeID')['json'])\n",
    "    source_df['Archive Identifier'] = source_df[node_mapping['to']].map(lookup_archives_df.set_index('brocadeID')['ark'])\n",
    "\n",
    "    \n",
    "    save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "    #source_df.to_csv('out/%s_%s_Group_%s.csv' % (resource_model, card, package), index=False)\n",
    "    del source_df\n",
    "    del source_list\n",
    "    del obj_list\n",
    "    return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edbbc6a-5de2-4503-b593-956d5f3edfd8",
   "metadata": {},
   "source": [
    "## Acquisitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a616fb09-09af-4c7d-96a1-6a26ead1e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_acquisitions(source_list, resource_model, package, dataset, card):\n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    \n",
    "    for obj in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in obj.keys():\n",
    "                obj_dict['ResourceID'] = obj['ResourceID']\n",
    "                obj_dict['brocade.id'] = obj['brocade.id']\n",
    "                obj_dict['card'] = card\n",
    "                obj_dict['order'] = obj['order']\n",
    "                obj_dict[node_mapping['to']] = obj[node_mapping['from']][0]['value']\n",
    "                obj_list.append(obj_dict)\n",
    "                obj_dict = {}\n",
    "    source_df = pd.DataFrame(obj_list)\n",
    "    number_of_records = len(obj_list)\n",
    "    source_df['Acquisition Widget'] = source_df[node_mapping['to']].map(lookup_acquisitions_df.set_index('brocadeID')['json'])\n",
    "    source_df['Acquisition Identifier'] = source_df[node_mapping['to']].map(lookup_acquisitions_df.set_index('brocadeID')['ark'])\n",
    "\n",
    "    \n",
    "    save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "    #source_df.to_csv('out/%s_%s_Group_%s.csv' % (resource_model, card, package), index=False)\n",
    "    del source_df\n",
    "    del source_list\n",
    "    del obj_list\n",
    "\n",
    "    return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695cdcd9-65ad-4717-b1c8-025f79c35683",
   "metadata": {},
   "source": [
    "## Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa14be03-46a5-41ab-a9d7-a0be44036caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conditions(source_list, resource_model, package, dataset, card, model_class):\n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    \n",
    "    for obj in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in obj.keys():\n",
    "                source_value = obj[node_mapping['from']][0]['value']\n",
    "                obj_dict.update(make_default_dict(obj, card, source_value, node_mapping['from'], model_class))\n",
    "                obj_dict[node_mapping['to']] = obj[node_mapping['from']][0]['value']\n",
    "                if 'function' in node_mapping.keys():\n",
    "                    _type = lookup_column_value(obj[node_mapping['from']][0]['value'], \n",
    "                                            node_mapping['function']['args']['lookup_file'],\n",
    "                                            card, obj['brocade.id'], node_mapping['from'])\n",
    "                    obj_dict[node_mapping['to']] = _type\n",
    "                    \n",
    "                if 'constants' in node_mapping.keys():\n",
    "                    constants = get_mark_type(mappings, node_mapping['from'])\n",
    "                    for _const, _value in constants.items():\n",
    "                        obj_dict[_const] = _value\n",
    "                obj_list.append(obj_dict)\n",
    "                obj_dict = {}\n",
    "\n",
    "    source_df = pd.DataFrame(obj_list)\n",
    "    number_of_records = len(obj_list)\n",
    "    save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "    del source_df\n",
    "    return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b1818-cfe9-4965-a6d3-006527c89549",
   "metadata": {},
   "source": [
    "## Dimension Source Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "148f773e-aa5e-4430-8550-809513207b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dimension_source_notes(source_list, resource_model, package, dataset, card):\n",
    "    mapping = multi_value_mappings['e22_Dimensions']\n",
    "    columns = get_from_keys(mapping)\n",
    "    dim_dict = {}\n",
    "    dim_list = []\n",
    "    const_dict = {}\n",
    "    const_list = []\n",
    "    \n",
    "    for dim in source_list:\n",
    "        for node_mapping in mapping['node_mappings']:\n",
    "            if node_mapping['from'] in dim.keys():\n",
    "                source_value = dim[node_mapping['from']][0]['value']\n",
    "                dim_dict.update(make_default_dict(dim, card, source_value, node_mapping['from'], 'e22'))\n",
    "                constants = get_constant_type(mapping, node_mapping['from'])\n",
    "                for _type, _value in constants.items():\n",
    "                    if _type != 'unit':\n",
    "                        const_list.append(_type)\n",
    "                _dimension = dim[node_mapping['from']][0]['value']\n",
    "                _constants = ', '.join(map(str, const_list))\n",
    "                _source_field = node_mapping['from']\n",
    "                const_list = []\n",
    "                dim_dict['Dimension Source Note'] = '%s - %s, %s' % (_dimension, _constants, _source_field)\n",
    "                dim_list.append(dim_dict)\n",
    "                dim_dict = {}\n",
    "\n",
    "    source_df = pd.DataFrame(dim_list)\n",
    "    number_of_records = len(dim_list)\n",
    "\n",
    "    save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "    del source_df\n",
    "    del source_list\n",
    "    del dim_list\n",
    "\n",
    "    return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58420f92-43c8-49a0-812a-0c083c664b5e",
   "metadata": {},
   "source": [
    "## Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da970627-005d-4e6a-b183-2278055da966",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OBS \n",
    "\n",
    "### {\"from\": \"adm.note\", \"to\": \"Annotation\",\n",
    "## ONLY IF DATASET = LH\n",
    "\n",
    "\n",
    "\n",
    "def check_constant(card, _key, _constant):\n",
    "    for item in multi_value_mappings[card]['node_mappings']:\n",
    "        if item['from'] == _key:\n",
    "            return item['constants'][0][_constant]\n",
    "\n",
    "def make_annotations(source_list, resource_model, package, dataset, card, model_class):\n",
    "\n",
    "    anno_dict = {}\n",
    "    anno_list = []\n",
    "    mappings = multi_value_mappings[card]\n",
    "\n",
    "    for anno in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in anno.keys():\n",
    "                source_value = anno[node_mapping['from']][0]['value']\n",
    "                if 'include_codes' in node_mapping.keys(): \n",
    "                    if source_value in node_mapping['include_codes']:\n",
    "                        anno_dict.update(make_default_dict(anno, card, source_value, node_mapping['from'], model_class))\n",
    "                        if anno[node_mapping['from']][0]['value'] == 'acai':\n",
    "                            anno_dict['Annotation'] = 'Alfabetische index'\n",
    "                        else:\n",
    "                            anno_dict['Annotation'] = anno[node_mapping['from']][0]['value']\n",
    "                        if 'constants' in node_mapping.keys():\n",
    "                            constants = get_mark_type(mappings, node_mapping['from'])\n",
    "                            for _type, _value in constants.items():\n",
    "                                anno_dict[_type] = _value\n",
    "                else: # no include codes\n",
    "                    anno_dict.update(make_default_dict(anno, card, source_value, node_mapping['from'], model_class))\n",
    "                    anno_dict['Annotation'] = anno[node_mapping['from']][0]['value']\n",
    "                    if 'constants' in node_mapping.keys():\n",
    "                        constants = get_mark_type(mappings, node_mapping['from'])\n",
    "                        for _type, _value in constants.items():\n",
    "                            anno_dict[_type] = _value\n",
    "                    \n",
    "                                \n",
    "        \n",
    "        anno_list.append(anno_dict)\n",
    "        anno_dict = {}\n",
    "\n",
    "    source_df = pd.DataFrame(anno_list)\n",
    "    \n",
    "    ## OBS \n",
    "\n",
    "    ### {\"from\": \"adm.note\", \"to\": \"Annotation\"} if LH. - > Filter out if not in LH\n",
    "    if package in ['pkg_mpm', 'pkg_rub']:\n",
    "        source_df = source_df[source_df['source_field']!='adm.note']\n",
    "    \n",
    "    number_of_records = len(source_df)\n",
    "    #source_df.sort_values(by=['ResourceID', 'card', 'order'])\n",
    "    \n",
    "    # save\n",
    "    if number_of_records > 0:\n",
    "        save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "    \n",
    "    del source_df\n",
    "    del source_list\n",
    "    del anno_list    \n",
    "    return '%s saved: %s' % (card, number_of_records)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf82a0-a9cf-45e9-84c2-99bcaeb26b98",
   "metadata": {},
   "source": [
    "## Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb35208a-1356-4adf-a6e2-c609cb9a810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_list = []\n",
    "replace_dict_x = {\"X\": \"x\",\n",
    "                \"A4\": \"29,7x21,0\",\n",
    "                \"a4\": \"29,7x21,0\",\n",
    "                \"A 4\": \"29,7x21,0\",\n",
    "                \"A3\": \"42,0x29,7\",\n",
    "                \"35mm\": \"2,4x3,6\",\n",
    "                \"35 mm\": \"2,4x3,6\",\n",
    "                \"Quarto\": \"29,7x21,0\",                                    \n",
    "                \"quarto\": \"29,7x21,0\",                                    \n",
    "                \"cm\": \"\",\n",
    "                \" x \": \"x\", \n",
    "                \",\": \".\",\n",
    "                \" \": \"\",\n",
    "                \".5.\": \".5\",\n",
    "                \"xxx\":\"x\",\n",
    "                \"xx\":\"x\",\n",
    "                \"'\": \",\",\n",
    "                \",7,\":\",7\",\n",
    "                \",,\":\",\",\n",
    "                \"h\":\"\",\n",
    "                \"b\":\"\",\n",
    "                \"d\":\"\",\n",
    "    } \n",
    "\n",
    "\n",
    "replace_dict_hbd = {\"X\": \"x\",\n",
    "                \"A4\": \"29,7x21,0\",\n",
    "                \"a4\": \"29,7x21,0\",\n",
    "                \"A 4\": \"29,7x21,0\",\n",
    "                \"A3\": \"42,0x29,7\",\n",
    "                \"35mm\": \"2,4x3,6\",\n",
    "                \"35 mm\": \"2,4x3,6\",\n",
    "                \"Quarto\": \"29,7x21,0\",    \n",
    "                \"quarto\": \"29,7x21,0\",                                    \n",
    "                \"cm\": \"\",\n",
    "                \" x \": \"x\", \n",
    "                \",\": \".\",\n",
    "                \" \": \"\",\n",
    "                \"'\": \",\",\n",
    "                \",,\":\",\",\n",
    "                \"x b\": \"x\",\n",
    "                \"x d\": \"x\",                    \n",
    "                \"h\":\"\",\n",
    "                \"b\":\"x\",\n",
    "                \"d\":\"x\",\n",
    "                \"H\":\"\",\n",
    "                \"B\":\"x\",\n",
    "                \"D\":\"x\",\n",
    "                \"xxx\":\"x\",\n",
    "                \"xx\":\"x\",\n",
    "    }\n",
    "\n",
    "def lookup_dimension_type(card, source_field, list_for_typing):\n",
    "\n",
    "    for item in multi_value_mappings[card]['node_mappings']:\n",
    "        if item['from'] == source_field:\n",
    "            for dim_to_type in list_for_typing:\n",
    "                dim_to_type['type'] = item['constants'][dim_to_type['type']]\n",
    "                dim_to_type['unit'] = item['constants']['unit']\n",
    "\n",
    "    return list_for_typing    \n",
    "\n",
    "def lookup_dimension_type_single(card, source_field, list_for_typing):\n",
    "\n",
    "    for item in multi_value_mappings[card]['node_mappings']:\n",
    "        if item['from'] == source_field:\n",
    "            for dim_to_type in list_for_typing:\n",
    "                dim_to_type['type'] = item['constants']['dimension']\n",
    "                dim_to_type['unit'] = item['constants']['unit']\n",
    "\n",
    "    return list_for_typing    \n",
    "\n",
    "\n",
    "def transform_cm_to_mm(_dimensions_list):\n",
    "    \n",
    "    for dim in _dimensions_list:        \n",
    "        dim['value'] = int(dim['value'] * 10)\n",
    "    return _dimensions_list\n",
    "\n",
    "def split_height_width_depth(dim_value, dataset, brocade_id, source_field, transform, card, replacedict):\n",
    "    \n",
    "    dimensions_list = []\n",
    "    \n",
    "    repl_value = dim_value    \n",
    "    for k, v in replacedict.items():\n",
    "        repl_value = repl_value.replace(k, v)\n",
    "    replaced_value = repl_value.strip()\n",
    "        \n",
    "    try:\n",
    "        split_dim = replaced_value.split('x')\n",
    "        split_dim_0 = float(split_dim[0])\n",
    "        split_dim_1 = float(split_dim[1])\n",
    "        split_dim_2 = float(split_dim[2])\n",
    "        \n",
    "        dimensions_list =  [{'value': split_dim_0, 'type': 'height'}, \n",
    "                {'value': split_dim_1, 'type': 'width' },\n",
    "                {'value': split_dim_2, 'type': 'depth' },\n",
    "               ]\n",
    "        if transform == \"True\":\n",
    "            dimensions_list = transform_cm_to_mm(dimensions_list)\n",
    "        for comma_float in dimensions_list:\n",
    "            comma_float['value'] = str(comma_float['value']).replace('.', ',')\n",
    " \n",
    "        typed_dimensions = lookup_dimension_type(card, source_field, dimensions_list)    \n",
    "        return typed_dimensions\n",
    "    \n",
    "    except:\n",
    "#        error = ['%s,%s,%s,%s,height x width x depth,split and float check failed' % (dataset, brocade_id, source_field, dim_value)]\n",
    "        error = [dataset,brocade_id,source_field,dim_value,'height x width x depth','split and float check failed']\n",
    "        #print(error)\n",
    "        #print('source:', dim_value, 'cleaned:',replaced_value)\n",
    "        error_list.append(error)\n",
    "        return False\n",
    "\n",
    "def split_height_width(dim_value, dataset, brocade_id, source_field, transform, card):\n",
    "    \n",
    "    dimensions_list = []\n",
    "    \n",
    "    repl_value = dim_value    \n",
    "    for k, v in replace_dict_x.items():\n",
    "        repl_value = repl_value.replace(k, v)\n",
    "    replaced_value = repl_value.strip()\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        split_dim = replaced_value.split('x')\n",
    "        split_dim_0 = float(split_dim[0])\n",
    "        split_dim_1 = float(split_dim[1])\n",
    "        \n",
    "        dimensions_list =  [{'value': split_dim_0, 'type': 'height'}, \n",
    "                {'value': split_dim_1, 'type': 'width' }\n",
    "               ]\n",
    "        \n",
    "        if transform == \"True\":\n",
    "            dimensions_list = transform_cm_to_mm(dimensions_list)   \n",
    "            for comma_float in dimensions_list:\n",
    "                comma_float['value'] = str(comma_float['value']).replace('.', ',')\n",
    "        elif transform == \"False\":\n",
    "            if source_field == 'kenmerken.lhformatph' and split_dim_0 == 2.4 and split_dim_1 == 3.6:\n",
    "                #print(brocade_id, source_field, split_dim_0, split_dim_1)\n",
    "                dimensions_list = transform_cm_to_mm(dimensions_list)   \n",
    "                for comma_float in dimensions_list:\n",
    "                    comma_float['value'] = str(comma_float['value']).replace('.', ',')                \n",
    "            else:\n",
    "                for comma_float in dimensions_list:\n",
    "                    comma_float['value'] = str(comma_float['value']).replace('.0', '')            \n",
    "            \n",
    "            \n",
    "        typed_dimensions = lookup_dimension_type(card, source_field, dimensions_list)    \n",
    "        return typed_dimensions\n",
    "    \n",
    "    except:\n",
    "        #error = ['%s,%s,%s,%s,height x width,split and float check failed' % (dataset, brocade_id, source_field, dim_value)]\n",
    "        error = [dataset,brocade_id,source_field,dim_value,'height x width','split and float check failed']\n",
    "        #print(error)\n",
    "        #print(dim_value)\n",
    "        error_list.append(error)\n",
    "        return False\n",
    "\n",
    "def split_diameter(dim_value, dataset, brocade_id, source_field, transform, card):\n",
    "    \n",
    "    dimensions_list = []\n",
    "    \n",
    "    repl_value = dim_value    \n",
    "    for k, v in replace_dict_x.items():\n",
    "        repl_value = repl_value.replace(k, v)\n",
    "    replaced_value = repl_value.strip()\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        split_dim = float(replaced_value)\n",
    "        \n",
    "        dimensions_list =  [{'value': split_dim, 'type': 'diameter'}]\n",
    "        if transform == \"True\":\n",
    "            dimensions_list = transform_cm_to_mm(dimensions_list)        \n",
    "        for comma_float in dimensions_list:\n",
    "            comma_float['value'] = str(comma_float['value']).replace('.', ',')\n",
    "            \n",
    "        typed_dimensions = lookup_dimension_type(card, source_field, dimensions_list)    \n",
    "        return typed_dimensions\n",
    "    \n",
    "    except:\n",
    "        #error = ['%s,%s,%s,%s,height x width,split and float check failed' % (dataset, brocade_id, source_field, dim_value)]\n",
    "        error = [dataset,brocade_id,source_field,dim_value,'diameter','split and float check failed']\n",
    "        #print(error)\n",
    "        #print(dim_value)\n",
    "        error_list.append(error)\n",
    "        return False\n",
    "        \n",
    "def split_single(dim_value, dataset, brocade_id, source_field, transform, card):\n",
    "    #print('single', brocade_id, dim_value)\n",
    "    dimensions_list = []\n",
    "    \n",
    "    repl_value = dim_value    \n",
    "    for k, v in replace_dict_x.items():\n",
    "        repl_value = repl_value.replace(k, v)\n",
    "    replaced_value = repl_value.strip()\n",
    "\n",
    "\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        split_dim = float(replaced_value)\n",
    "        dimensions_list =  [{'value': split_dim}]\n",
    "        \n",
    "        if transform == \"True\":\n",
    "            dimensions_list = transform_cm_to_mm(dimensions_list)        \n",
    "        for comma_float in dimensions_list:\n",
    "            comma_float['value'] = str(comma_float['value']).replace('.', ',')\n",
    "            \n",
    "        typed_dimensions = lookup_dimension_type_single(card, source_field, dimensions_list)\n",
    "        #print(typed_dimensions, brocade_id, dimensions_list)\n",
    "        \n",
    "        return typed_dimensions\n",
    "    \n",
    "    except:\n",
    "        #error = ['%s,%s,%s,%s,height x width,split and float check failed' % (dataset, brocade_id, source_field, dim_value)]\n",
    "        error = [dataset,brocade_id,source_field,dim_value,'diameter','split and float check failed']\n",
    "        print(error)\n",
    "        #print(dim_value)\n",
    "        error_list.append(error)\n",
    "        return False\n",
    "            \n",
    "def make_dimensions(source_list, resource_model, package, dataset, card):\n",
    "\n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    dim_dict = {}\n",
    "    dim_list = []\n",
    "    \n",
    "    for dim in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in dim.keys():\n",
    "                dimension_value = dim[node_mapping['from']][0]['value']\n",
    "                if node_mapping['type'] == \"height_width_depth\":\n",
    "                    splitted_dimensions = split_height_width_depth(dimension_value, dataset, dim['brocade.id'], \n",
    "                                             node_mapping['from'], node_mapping['transform'], card, replace_dict_x)\n",
    "                if node_mapping['type'] == \"height_width_depth_hbd\":\n",
    "                    splitted_dimensions = split_height_width_depth(dimension_value, dataset, dim['brocade.id'], \n",
    "                                             node_mapping['from'], node_mapping['transform'], card, replace_dict_hbd)\n",
    "                elif node_mapping['type'] == \"height_width\":\n",
    "                    splitted_dimensions = split_height_width(dimension_value, dataset, dim['brocade.id'], \n",
    "                                             node_mapping['from'], node_mapping['transform'], card)\n",
    "                elif node_mapping['type'] == \"diameter\":\n",
    "                    splitted_dimensions = split_diameter(dimension_value, dataset, dim['brocade.id'], \n",
    "                                             node_mapping['from'], node_mapping['transform'], card)\n",
    "                elif node_mapping['type'] == \"single\":\n",
    "                    splitted_dimensions = split_single(dimension_value, dataset, dim['brocade.id'], \n",
    "                                             node_mapping['from'], node_mapping['transform'], card)\n",
    "                #print(splitted_dimensions)\n",
    "                if splitted_dimensions:\n",
    "                    for typed_dim in splitted_dimensions:\n",
    "                        dim_dict['ResourceID'] = dim['ResourceID']\n",
    "                        dim_dict['brocade.id'] = dim['brocade.id']\n",
    "                        dim_dict['card'] = card\n",
    "                        dim_dict['source_field'] = node_mapping['from']\n",
    "                        dim_dict['Dimension Value'] = typed_dim['value']\n",
    "                        dim_dict['Dimension Type'] = typed_dim['type']\n",
    "                        dim_dict['Dimension Measurement Unit'] = typed_dim['unit']\n",
    "                        dim_list.append(dim_dict)\n",
    "                        dim_dict = {}\n",
    "\n",
    "    source_df = pd.DataFrame(dim_list)\n",
    "    number_of_records = len(dim_list)\n",
    "#    source_df['Dimension Type'] = source_df['Dimension Type'].astype(int)\n",
    "    \n",
    "    # save\n",
    "    save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "    #source_df.to_csv('out/' + dataset + '_dims.csv')\n",
    "    del source_df\n",
    "    del source_list\n",
    "    del dim_list\n",
    "\n",
    "    return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11901e5c-5052-4a12-b966-c966b7d79ab5",
   "metadata": {},
   "source": [
    "## Make Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea88f2ac-b5c2-44fc-8585-40664380b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_single(source_list, resource_model, package, dataset, card):\n",
    "    \n",
    "    single_df = pd.DataFrame(source_list)\n",
    "    #single_df.to_csv('out/' + package + 'single.csv', index=False)\n",
    "\n",
    "    mappings = multi_value_mappings[card]\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    \n",
    "    for obj in source_list:\n",
    "        obj_dict['ResourceID'] = obj['ResourceID']\n",
    "        obj_dict['brocade.id'] = obj['brocade.id']\n",
    "        obj_dict['card'] = 'AA' ## dummy for sorting later on\n",
    "        obj_dict['order'] = '0'\n",
    "\n",
    "        if 'mpm' in obj['brocade.type'][0]['value']:\n",
    "            obj_dict['Keeper'] = 'Museum Plantin-Moretus'\n",
    "        elif 'lh' in obj['brocade.type'][0]['value']:\n",
    "            obj_dict['Keeper'] = 'Letterenhuis'\n",
    "        elif 'rub' in obj['brocade.type'][0]['value']:\n",
    "            obj_dict['Keeper'] = 'Rubenianum'\n",
    "\n",
    "        obj_dict['Object Number'] = obj['brocade.id'].replace('_#', ':m')    \n",
    "        obj_dict['Object Number Type'] = 'record identifiers'\n",
    "        obj_dict['Object Identifier'] = add_ark_identifier_by_row_value(package, obj['ResourceID'])\n",
    "        obj_dict['Object Identifier Type'] = 'object identifier'\n",
    "        obj_list.append(obj_dict)\n",
    "        obj_dict = {}\n",
    "    \n",
    "    source_df = pd.DataFrame(obj_list)\n",
    "    number_of_records = len(obj_list)    \n",
    "    \n",
    "    save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "    del source_df\n",
    "    del source_list\n",
    "    del obj_list\n",
    "\n",
    "    return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99f581b-df50-4350-be9a-bf7f8c5d82b6",
   "metadata": {},
   "source": [
    "## Production Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e69b6cfb-5518-4efe-9792-9440dd751ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plain_ire(source_list, resource_model, package, dataset, card, model_class):\n",
    "    \n",
    "    m_dict = {}\n",
    "    node_mapping_dict = {}\n",
    "    isaar_list = []\n",
    "    card = card\n",
    "    mappings = multi_value_mappings[card]\n",
    "    print(dataset)\n",
    "    \n",
    "    #print(mappings['include_codes'])\n",
    "    for ire in source_list:\n",
    "        if ire['ire.type'][0]['value'] in mappings['include_codes']: \n",
    "            if model_class == 'e73':\n",
    "                m_dict['ResourceID'] = ire['ResourceID']\n",
    "            elif model_class == 'e22':   \n",
    "                m_dict['ResourceID'] = ire['ResourceID']\n",
    "            m_dict['brocade.id'] = ire['brocade.id']\n",
    "            m_dict['card'] = card\n",
    "            m_dict['order'] = ire['order']\n",
    "            m_dict['ire.isaar.uuid'] = ire['ire.isaar'][0]['uuid']\n",
    "            au_code = ire['ire.isaar'][0]['value']\n",
    "            \n",
    "            #Quick fix for MPM\n",
    "            if dataset == 'mpmtk':\n",
    "                m_dict['ire.isaar.id'] = au_code + ':1'\n",
    "            \n",
    "            #Quick fix for LH\n",
    "            if dataset in ['lhtk', 'lhsc', 'lhhs']:                \n",
    "                if au_code == 'au::46030:1':                    \n",
    "                    m_dict['ire.isaar.id'] = 'au::46030:1'                                \n",
    "                elif au_code == 'au::114141:2':                    \n",
    "                    m_dict['ire.isaar.id'] = 'au::114141:2'  \n",
    "                elif au_code == 'au::18121:1':                    \n",
    "                    m_dict['ire.isaar.id'] = 'au::18121:1'                                                                                                                                                  \n",
    "                elif au_code == 'au::5420:3':                    \n",
    "                    m_dict['ire.isaar.id'] = 'au::5420:3'                                                                                                                                                  \n",
    "                elif au_code == 'au::5420:3':                    \n",
    "                    m_dict['ire.isaar.id'] = 'au::5420:3'                     \n",
    "                elif au_code == 'au::47769:1':                    \n",
    "                    m_dict['ire.isaar.id'] = 'au::47769:1' \n",
    "                elif au_code == 'au::33117:1':                    \n",
    "                    m_dict['ire.isaar.id'] = 'au::33117:1' \n",
    "                elif au_code == 'au::40049:1':                    \n",
    "                    m_dict['ire.isaar.id'] = 'au::40049:1' \n",
    "                elif au_code == 'au::6645:1':                    \n",
    "                    m_dict['ire.isaar.id'] = 'au::6645:1' \n",
    "                elif au_code == 'au::34521:1':                    \n",
    "                    m_dict['ire.isaar.id'] = 'au::34521:1' \n",
    "                elif au_code == 'au::25344:1':                    \n",
    "                    m_dict['ire.isaar.id'] = 'au::25344:1' \n",
    "                elif au_code == 'au::45684:1':                    \n",
    "                    m_dict['ire.isaar.id'] = 'au::45684:1' \n",
    "                elif au_code == 'au::7356:1':                    \n",
    "                    m_dict['ire.isaar.id'] = 'au::7356:1' \n",
    "                elif au_code == 'au::12170:1':                    \n",
    "                    m_dict['ire.isaar.id'] = 'au::12170:1' \n",
    "                                        \n",
    "\n",
    "                else:\n",
    "                    m_dict['ire.isaar.id'] = au_code + ':1'\n",
    "            \n",
    "            # hack for Affiche\n",
    "            if dataset == 'lhps': \n",
    "                if au_code == 'au::20710:2:N':                    \n",
    "                    m_dict['ire.isaar.id'] = 'au::20710:2'                                \n",
    "                elif au_code == 'au::5420:3':                    \n",
    "                    m_dict['ire.isaar.id'] = 'au::5420:3'                \n",
    "                elif au_code == '114141:2':                    \n",
    "                    m_dict['ire.isaar.id'] = '114141:2'                  \n",
    "                elif au_code == '114141:2':                    \n",
    "                    m_dict['ire.isaar.id'] = '114141:2'  \n",
    "                elif au_code == 'au::18121:1':                    \n",
    "                    m_dict['ire.isaar.id'] = 'au::18121:1'                                                          \n",
    "                else:\n",
    "                    m_dict['ire.isaar.id'] = au_code + ':1'                                        \n",
    "                \n",
    "            m_dict['source role'] = ire['ire.type'][0]['value']\n",
    "            m_dict['ire.role'] = ire['ire.type'][0]['value']\n",
    "            if 'ire.markdown' in ire.keys():\n",
    "                m_dict['ire.markdown'] = ire['ire.markdown'][0]['value']\n",
    "            isaar_list.append(m_dict)\n",
    "            m_dict = {}\n",
    "        \n",
    "    dataset_df = pd.DataFrame(isaar_list)\n",
    "    number_of_records = len(isaar_list)\n",
    "    if len(dataset_df) > 0:            \n",
    "        #dataset_df['ire.identifier'] = dataset_df['ire.isaar.id'].map(lookup_imaginary_df.set_index('archesID')['ark'])\n",
    "        \n",
    "        dataset_df['ire.widget'] = dataset_df['ire.isaar.id'].map(lookup_imaginary_df.set_index('archesID')['json'])\n",
    "        dataset_df['ire.role'] = dataset_df['ire.role'].map(lookup_relation_df.set_index('code')['concept'])\n",
    "        \n",
    "    \n",
    "        for node_mappings in mappings['node_mappings']:\n",
    "            node_mapping_dict[node_mappings['from']] = node_mappings['to']        \n",
    "        dataset_df = dataset_df.rename(columns=node_mapping_dict)\n",
    "        \n",
    "        save_it = save_mapped_set(dataset_df, package, resource_model, card)\n",
    "\n",
    "    del dataset_df\n",
    "    del source_list\n",
    "    del isaar_list\n",
    "\n",
    "    return '%s saved: %s' % (card, number_of_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ac3dc-5c7c-43fa-9336-ea94b150f45a",
   "metadata": {},
   "source": [
    "## Object Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64b5d856-121d-43b1-aaf3-7932d0fafc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_object_types(single_list, source_list, resource_model, package, dataset, card):\n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    no_obj_dict = {}\n",
    "    no_inc_dict = {}\n",
    "    obj_list = []\n",
    "    default_dict_items = {}\n",
    "    single_id_list = []\n",
    "    source_id_list = []\n",
    "    no_type_list = []\n",
    "    no_include_codes_list = []\n",
    "    \n",
    "    for source in source_list:\n",
    "        source_id_list.append(source['brocade.id'])\n",
    "\n",
    "    for single in single_list:\n",
    "        single_id_list.append(single['brocade.id'])\n",
    "    \n",
    "    # records without object codes:        \n",
    "    no_type_list = list(set(single_id_list) - set(source_id_list))\n",
    "        \n",
    "#    print('single', len(single_list))\n",
    "#    print('has types', len(source_list))\n",
    "#    print('no types', len(no_type_list), no_type_list)\n",
    "    \n",
    "    ## Generate from the ones in the mapping and that have an object code: source_list\n",
    "    for obj in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in obj.keys():\n",
    "                source_value = obj[node_mapping['from']][0]['value']\n",
    "                if 'include_codes' in node_mapping.keys(): \n",
    "                    if source_value in node_mapping['include_codes']:\n",
    "                        obj_dict.update(make_default_dict(obj, card, source_value, node_mapping['from'], 'e22'))\n",
    "                        if 'function' in node_mapping.keys():\n",
    "                            if node_mapping['function']['name'] == 'column_lookup':\n",
    "                                source_value = obj[node_mapping['from']][0]['value']                                \n",
    "                                if obj['dataset'] == 'rubdoc' and source_value == 'dcdo':\n",
    "                                    lookup_value = 'dcdo_rub'\n",
    "                                else:\n",
    "                                    lookup_value = source_value\n",
    "                                _type = lookup_column_value(lookup_value, \n",
    "                                        node_mapping['function']['args']['lookup_file'],\n",
    "                                        card, obj['brocade.id'], node_mapping['from'])\n",
    "                                obj_dict[node_mapping['to']] = _type                            \n",
    "                        obj_list.append(obj_dict)\n",
    "                        obj_dict = {}\n",
    "                    else:\n",
    "                        no_include_codes_list.append(obj['brocade.id'])\n",
    "                        \n",
    "\n",
    "    single_dict = {item['brocade.id']: item for item in single_list}    \n",
    "    for no_obj in no_type_list:            \n",
    "        if no_obj in single_dict:\n",
    "            no_obj_single = single_dict[no_obj]\n",
    "            #print(f\"Match found: {no_obj} and {no_obj_single}\")\n",
    "        \n",
    "            no_obj_dict.update(make_default_dict(no_obj_single, card, 'none', 'default_object_type', 'e22'))\n",
    "            _dataset = single['brocade.id'].split(':')[1]\n",
    "            #print('no_type_list: ', single['brocade.id'],_dataset)#\n",
    "            default_type = lookup_column_value(_dataset, \n",
    "                                        'default_object_types.csv',\n",
    "                                        card, single['brocade.id'], 'none')\n",
    "            no_obj_dict['Object Type'] = default_type                            \n",
    "            obj_list.append(no_obj_dict)\n",
    "            no_obj_dict = {}\n",
    "    \n",
    "    print(len(source_list), len(no_type_list), len(no_include_codes_list))\n",
    "\n",
    "    for no_inc in no_include_codes_list:\n",
    "        if no_inc in single_dict:\n",
    "            no_inc_single = single_dict[no_inc]\n",
    "\n",
    "            no_inc_dict.update(make_default_dict(no_inc_single, card, 'none', 'default_object_type', 'e22'))\n",
    "            _dataset = single['brocade.id'].split(':')[1]\n",
    "            default_type = lookup_column_value(_dataset, \n",
    "                                        'default_object_types.csv',\n",
    "                                        card, single['brocade.id'], 'none')\n",
    "            no_inc_dict['Object Type'] = default_type                            \n",
    "            obj_list.append(no_inc_dict)\n",
    "            no_inc_dict = {}\n",
    "             \n",
    "    \n",
    "    source_df = pd.DataFrame(obj_list)\n",
    "    \n",
    "    if package == 'pkg_rub' and resource_model != 'Iconografie':\n",
    "        if resource_model == 'Brief':\n",
    "            drop_codes_list = [\"dtrvat\",\"dtrvbm\",\"dtrvbu\",\"dtrvfk\",\"dtrvfo\",\"dtrvhs\",\"dtrvka\",\"dtrvnt\",\"dtrvod\",\"dtrvpr\",\"dtrvre\",\"dtrvrg\",\"dtrvty\"]        \n",
    "        elif resource_model == 'Foto':        \n",
    "            drop_codes_list = [\"dtrvnt\",\"dtrvre\"]            \n",
    "        elif resource_model == 'Tekstdrager':                    \n",
    "            drop_codes_list = [\"dtrvfo\"]    \n",
    "        source_df = source_df.loc[source_df['source_code'].isin(drop_codes_list) == False]\n",
    "        drop_codes_list = []\n",
    "        \n",
    "    number_of_records = len(source_df)\n",
    "    \n",
    "    save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "    del source_df\n",
    "    del source_list\n",
    "    del no_type_list\n",
    "    return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4d0a41-a52f-4e93-90b6-44d5ad3a21f0",
   "metadata": {},
   "source": [
    "## Acquisition Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a31f7691-590e-40c2-8301-39ec74d20535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_acquisition_numbers(source_list, resource_model, package, dataset, card):\n",
    "\n",
    "    dim_dict = {}\n",
    "    dim_list = []\n",
    "    _acq_number_list = []\n",
    "    _acq_number_dict = []\n",
    "\n",
    "    card = card\n",
    "    mapping = multi_value_mappings[card]\n",
    "    \n",
    "    for dim in source_list:\n",
    "        for node_mapping in mapping['node_mappings']:\n",
    "            if node_mapping['from'] in dim.keys():\n",
    "                if package == 'pkg_mpm':\n",
    "                    pass\n",
    "#                    order = 1\n",
    "#                    for _acq_number in dim['adm.inv']:                    \n",
    "#                        if order == 2:\n",
    "#                            dim_dict['ResourceID'] = dim['ResourceID']\n",
    "#                            dim_dict['brocade.id'] = dim['brocade.id']\n",
    "#                            dim_dict['card'] = card\n",
    "#                            dim_dict[node_mapping['to']] = _acq_number['value']\n",
    "#                            _acq_number = ''\n",
    "#                            constants = get_constant_type(mapping, node_mapping['from'])\n",
    "#                            for _type, _value in constants.items():\n",
    "#                                dim_dict[_type] = _value\n",
    "#                            dim_list.append(dim_dict)\n",
    "#                            dim_dict = {}\n",
    "#                        order += 1 \n",
    "                else:\n",
    "\n",
    "                    dim_dict['ResourceID'] = dim['ResourceID']\n",
    "                    dim_dict['brocade.id'] = dim['brocade.id']\n",
    "                    dim_dict['card'] = card\n",
    "                    dim_dict['order'] = dim['order']\n",
    "                    for _acq_number in dim['adm.inv']:                    \n",
    "                        _acq_number_list.append(_acq_number['value'])\n",
    "                    _acq_number = ', '.join(_acq_number_list)\n",
    "                    dim_dict[node_mapping['to']] = _acq_number\n",
    "                    _acq_number_list = []\n",
    "                    _acq_number = ''\n",
    "                    constants = get_constant_type(mapping, node_mapping['from'])\n",
    "                    for _type, _value in constants.items():\n",
    "                        dim_dict[_type] = _value\n",
    "                    dim_list.append(dim_dict)\n",
    "                    dim_dict = {}\n",
    "                    \n",
    "                \n",
    "\n",
    "    source_df = pd.DataFrame(dim_list)\n",
    "    number_of_records = len(dim_list)    \n",
    "\n",
    "    # save\n",
    "    if len(source_df) > 0:\n",
    "        \n",
    "        # hack for truncated strings. in source\n",
    "        add_trail_df = pd.read_csv(lookup_path + 'adm-inv-commas-20231023.csv')\n",
    "        source_df.set_index('brocade.id', inplace=True)\n",
    "        add_trail_df.set_index('brocade.id', inplace=True)\n",
    "        source_df.update(add_trail_df)\n",
    "        source_df.reset_index(inplace=True)    \n",
    "        source_df = source_df[[\"ResourceID\",\"brocade.id\",\"card\",\"order\",\"Acquisition Number\",\"Acquisition Number Type\"]]            \n",
    "        \n",
    "        save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "\n",
    "    dim_dict = {}\n",
    "    dim_list = []\n",
    "    del source_df\n",
    "    del source_list\n",
    "\n",
    "    return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332c61a6-1dec-48a8-bb7e-30a044ca5c25",
   "metadata": {},
   "source": [
    "## make_production_technique_note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bee8455d-075e-4197-bffd-d4383a1705c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_production_technique_note(source_list, resource_model, package, dataset, card):\n",
    "    mappings = multi_value_mappings[card]\n",
    "    columns = get_from_keys(mappings)\n",
    "    obj_dict = {}\n",
    "    obj_list = []\n",
    "    default_dict_items = {}\n",
    "    \n",
    "    for obj in source_list:\n",
    "        for node_mapping in mappings['node_mappings']:\n",
    "            if node_mapping['from'] in obj.keys():\n",
    "                source_value = obj[node_mapping['from']][0]['value']\n",
    "                obj_dict.update(make_default_dict(obj, card, source_value, node_mapping['from'], 'e22'))\n",
    "                obj_dict[node_mapping['to']] = source_value\n",
    "                obj_list.append(obj_dict)\n",
    "                obj_dict = {}\n",
    "\n",
    "    source_df = pd.DataFrame(obj_list)\n",
    "    number_of_records = len(obj_list)\n",
    "    if number_of_records > 0:\n",
    "        save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "        del source_df\n",
    "        del source_list\n",
    "        return '%s saved: %s' % (card, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d1d02-5fa9-4801-a481-b5ebbd214b50",
   "metadata": {},
   "source": [
    "## Other identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14e29fd8-dc8d-4d34-a7c0-4087b0b9bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_other_identifier(source_list, resource_model, package, dataset, card):\n",
    "\n",
    "    dim_dict = {}\n",
    "    dim_list = []\n",
    "    _acq_number_list = []\n",
    "    _acq_number_dict = []\n",
    "\n",
    "    card = card\n",
    "    mapping = multi_value_mappings[card]\n",
    "    \n",
    "    for dim in source_list:\n",
    "        for node_mapping in mapping['node_mappings']:\n",
    "            if node_mapping['from'] in dim.keys():\n",
    "                if package == 'pkg_mpm': \n",
    "                    order = 1\n",
    "                    for _acq_number in dim['adm.inv']:                    \n",
    "                        if order == 2:\n",
    "                            dim_dict['ResourceID'] = dim['ResourceID']\n",
    "                            dim_dict['brocade.id'] = dim['brocade.id']\n",
    "                            dim_dict['card'] = card\n",
    "                            dim_dict[node_mapping['to']] = _acq_number['value']\n",
    "                            _acq_number = ''\n",
    "                            constants = get_constant_type(mapping, node_mapping['from'])\n",
    "                            for _type, _value in constants.items():\n",
    "                                dim_dict[_type] = _value\n",
    "                            dim_list.append(dim_dict)\n",
    "                            dim_dict = {}\n",
    "                        order += 1 \n",
    "\n",
    "    source_df = pd.DataFrame(dim_list)\n",
    "    number_of_records = len(dim_list)    \n",
    "\n",
    "    # save\n",
    "    if len(source_df) > 0:\n",
    "        save_it = save_mapped_set(source_df, package, resource_model, card)\n",
    "\n",
    "    dim_dict = {}\n",
    "    dim_list = []\n",
    "    del source_df\n",
    "    del source_list\n",
    "\n",
    "    return '%s saved: %s' % (card, number_of_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049051ad-ae03-41d8-aab8-0489d3ed3e0c",
   "metadata": {},
   "source": [
    "## Dams Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bdacae39-a2fe-4ead-82a8-64e2481934b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dams_dir = '../../source/static/dams/'\n",
    "def make_dams(resource_model, package, dataset):\n",
    "    read_name = dams_dir + 'e22_DamsLinks_' + package.split('_')[1] + '_' + resource_model + '.csv'\n",
    "    print(read_name)\n",
    "    save_name = '%s%s/%s/%s.csv' % (save_path, package.split('_')[1], resource_model, 'e22_DamsLinks')\n",
    "    print(save_name)\n",
    "    try:\n",
    "        shutil.copy2(read_name, save_name) \n",
    "        _return_msg = 'e22_DamsLinks saved'\n",
    "    except:\n",
    "        _return_msg = 'no dams file'\n",
    "    \n",
    "    return _return_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49af7a3-e878-4424-8527-343cd44e126d",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15b92b8c-4fb4-4c7b-ac65-8c40f8f7b7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iconografie\n",
      "-  pkg_mpm\n",
      "   -  mpmtk\n",
      "      -  e22_AcquisitionNumbers saved: 0\n",
      "      -  e22_Archives saved: 2\n",
      "      -  e22_Carrier saved: 19\n",
      "      -  e22_Dimensions saved: 38\n",
      "      -  e22_DimensionSourceNotes saved: 19\n",
      "      -  e22_Items saved: 19\n",
      "      -  e22_Marks saved: 11\n",
      "mpmtk\n",
      "      -  e22_ProductionActors saved: 2\n",
      "      -  e22_ProductionTechniques saved: 18\n",
      "      -  e22_Single saved: 19\n",
      "19 0 0\n",
      "      -  e22_ObjectTypes saved: 19\n",
      "      -  e22_OtherIdentifiers saved: 0\n",
      "      -  None\n",
      "Foto\n",
      "-  pkg_mpm\n",
      "   -  mpmph\n",
      "      -  e22_AcquisitionNumbers saved: 0\n",
      "      -  e22_AnnotationsMaterial saved: 0\n",
      "      -  e22_Archives saved: 97\n",
      "      -  e22_Dimensions saved: 12\n",
      "      -  e22_DimensionSourceNotes saved: 97\n",
      "      -  e22_Items saved: 91\n",
      "mpmph\n",
      "      -  e22_ProductionActors saved: 0\n",
      "      -  e22_Single saved: 97\n",
      "97 0 41\n",
      "      -  e22_ObjectTypes saved: 97\n",
      "      -  e22_OtherIdentifiers saved: 0\n",
      "      -  None\n",
      "Brief\n",
      "-  pkg_mpm\n",
      "   -  mpmbr\n",
      "      -  e22_AcquisitionNumbers saved: 0\n",
      "      -  e22_AnnotationsMaterial saved: 22\n",
      "      -  e22_Archives saved: 1200\n",
      "      -  e22_DocumentStatus saved: 14\n",
      "      -  e22_Items saved: 1200\n",
      "      -  e22_Materials saved: 1198\n",
      "mpmbr\n",
      "      -  e22_ProductionActors saved: 0\n",
      "      -  e22_ProductStadiums saved: 62\n",
      "      -  e22_ProductionTechniques saved: 1198\n",
      "      -  e22_ExtraProductionTechniques saved: 1\n",
      "      -  e22_Single saved: 1200\n",
      "1198 2 1197\n",
      "      -  e22_ObjectTypes saved: 1200\n",
      "      -  e22_OtherIdentifiers saved: 0\n",
      "      -  None\n",
      "Tekstdrager\n",
      "-  pkg_mpm\n",
      "   -  mpmak\n",
      "   -  mpmdc\n",
      "   -  mpmhs\n",
      "   -  mpmre\n",
      "      -  e22_AcquisitionNumbers saved: 0\n",
      "      -  e22_AnnotationsMaterial saved: 328\n",
      "      -  e22_Archives saved: 1135\n",
      "      -  e22_Carrier saved: 388\n",
      "      -  e22_Dimensions saved: 2222\n",
      "      -  e22_DimensionSourceNotes saved: 1112\n",
      "      -  e22_DocumentStatus saved: 106\n",
      "      -  e22_Formats saved: 2186\n",
      "      -  e22_HallmarkTypes saved: 149\n",
      "      -  e22_Items saved: 1137\n",
      "      -  e22_Marks saved: 285\n",
      "      -  e22_Materials saved: 1085\n",
      "mpmre\n",
      "      -  e22_ProductionActors saved: 0\n",
      "      -  e22_ProductStadiums saved: 110\n",
      "      -  e22_ProductionTechniques saved: 10\n",
      "      -  e22_Single saved: 1137\n",
      "638 499 0\n",
      "      -  e22_ObjectTypes saved: 1137\n",
      "      -  e22_OtherIdentifiers saved: 41\n",
      "      -  None\n",
      "---------\n",
      "2023-11-21 12:47:56.967618\n",
      "CPU times: user 6.56 s, sys: 1.72 s, total: 8.28 s\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "source_df = []\n",
    "record_dict = {}\n",
    "record_list = []\n",
    "records_list = []\n",
    "\n",
    "source_place_list = [\"rubhs\"]\n",
    "file_places_list = [\"lhps\"]\n",
    "\n",
    "\n",
    "for resource_model, packages in resource_model_list.items():\n",
    "    print(resource_model)    \n",
    "    for package, datasets in packages.items():\n",
    "        \n",
    "        ObjectTypes_List = []\n",
    "        DefaultObjectTypes_List = []\n",
    "        HallmarkTypes_list = []\n",
    "        Marks_list = []\n",
    "        Items_list = []\n",
    "        ProductStadiums_list = []\n",
    "        DocumentStatus_list = []\n",
    "        Carrier_list = []\n",
    "        Formats_list = []\n",
    "        \n",
    "        # FROM RDM\n",
    "        ProductionActors_list = []\n",
    "        ProductionTechniques_list = []\n",
    "        ExtraProductionTechniques_list = []\n",
    "        Materials_list = []\n",
    "        Colours_list = []\n",
    "        Acquisitions_list = []\n",
    "        Archives_list = []\n",
    "        Conditions_list = []\n",
    "        DimensionSourceNotes_list = []\n",
    "        AnnotationsMaterial_list = []\n",
    "        Dimensions_list = []\n",
    "        Single_list = []\n",
    "        AcquisitionNumbers_List = []\n",
    "\n",
    "        PlacesSource_list = []\n",
    "        ProductionTechniqueNote_list = []\n",
    "        OtherIdentifiers_list = []\n",
    "        \n",
    "        print('- ', package)\n",
    "        for dataset in datasets:\n",
    "            if dataset_size == 'all':\n",
    "                source_file_name = source_path + dataset + '.json'\n",
    "            else:\n",
    "                source_file_name = source_path + 'slices/' + dataset + '_' + dataset_size +'.json'\n",
    "\n",
    "            with open(source_file_name) as f:\n",
    "                records = json.load(f)\n",
    "                \n",
    "            print('   - ', dataset)\n",
    "            for record in records:\n",
    "                \n",
    "                for card, v in multi_value_mappings.items():\n",
    "                    brocade_id = record['ID']\n",
    "                    _order = brocade_id.split('_')[1].replace('#', '')\n",
    "                    brocade_uuid = record['groups']['adm']['Entries'][_order]['Fields']['uuid_mat'][0]['value']\n",
    "                    for group_key in get_group_keys(v): \n",
    "                        if group_key in record['groups'].keys():\n",
    "                            group_values = record['groups'][group_key]\n",
    "                            for order, item in group_values['Entries'].items():\n",
    "                                if any(key in item['Fields'].keys() for key in get_field_keys(v)):\n",
    "                                    record_dict['card'] = card\n",
    "                                    record_dict['order'] = order\n",
    "                                    record_dict['dataset'] = dataset\n",
    "                                    for _key, _value in item['Fields'].items():\n",
    "                                        record_dict['%s.%s' % (group_key, _key)] = _value\n",
    "                                    record_dict['ResourceID'] = brocade_uuid\n",
    "                                    record_dict['brocade.id'] = brocade_id\n",
    "\n",
    "                                    record_list.append(record_dict)\n",
    "                                    record_dict = {}   \n",
    "                                    \n",
    "                                if card == 'e22_ObjectTypes':\n",
    "                                    ObjectTypes_List.extend(record_list)                                    \n",
    "                                if card == 'e22_HallmarkTypes':\n",
    "                                    HallmarkTypes_list.extend(record_list)                                    \n",
    "                                if card == 'e22_Marks':\n",
    "                                    Marks_list.extend(record_list)  \n",
    "                                if card == 'e22_Items':\n",
    "                                    Items_list.extend(record_list)\n",
    "                                if card == 'e22_ProductStadiums':\n",
    "                                    ProductStadiums_list.extend(record_list)                                    \n",
    "                                if card == 'e22_DocumentStatus':\n",
    "                                    DocumentStatus_list.extend(record_list)                                    \n",
    "                                if card == 'e22_Carrier':\n",
    "                                    Carrier_list.extend(record_list)                                    \n",
    "                                if card == 'e22_Formats':\n",
    "                                    Formats_list.extend(record_list)\n",
    "\n",
    "                                # FROM RDM\n",
    "                                if card == 'e22_ProductionActors':\n",
    "                                    ProductionActors_list.extend(record_list)                                    \n",
    "                                if card == 'e22_ProductionTechniques':\n",
    "                                    ProductionTechniques_list.extend(record_list)                                    \n",
    "                                if card == 'e22_ExtraProductionTechniques':\n",
    "                                    ExtraProductionTechniques_list.extend(record_list)                                    \n",
    "                                if card == 'e22_Colours':\n",
    "                                    Colours_list.extend(record_list)                                    \n",
    "                                if card == 'e22_Materials':\n",
    "                                    Materials_list.extend(record_list)                                    \n",
    "                                if card == 'e22_Acquisitions':                    \n",
    "                                    Acquisitions_list.extend(record_list)\n",
    "                                if card == 'e22_Archives':\n",
    "                                    Archives_list.extend(record_list)\n",
    "                                if card == 'e22_Conditions':\n",
    "                                    Conditions_list.extend(record_list)                                    \n",
    "                                if card == 'e22_DimensionSourceNotes':\n",
    "                                    DimensionSourceNotes_list.extend(record_list)                                    \n",
    "                                if card == 'e22_AnnotationsMaterial':\n",
    "                                    AnnotationsMaterial_list.extend(record_list)\n",
    "                                if card == 'e22_Dimensions':\n",
    "                                    Dimensions_list.extend(record_list)                                    \n",
    "\n",
    "                                if card == 'e22_Single':\n",
    "                                    Single_list.extend(record_list)                                    \n",
    "                                if card == 'e22_AcquisitionNumbers':\n",
    "                                    AcquisitionNumbers_List.extend(record_list)                                    \n",
    "                                if card == 'e22_ProductionTechniqueNote':\n",
    "                                    ProductionTechniqueNote_list.extend(record_list)                                    \n",
    "                                if card == 'e22_OtherIdentifiers':\n",
    "                                    OtherIdentifiers_list.extend(record_list)                                    \n",
    "                                    \n",
    "                                    \n",
    "\n",
    "                                    \n",
    "                                record_list = []                                \n",
    "\n",
    "                                \n",
    "    \n",
    "        if len(Acquisitions_list) > 0:\n",
    "            Acquisitions = make_acquisitions(Acquisitions_list, resource_model, package, dataset, 'e22_Acquisitions')                    \n",
    "            Acquisitions_list = []\n",
    "            print('      - ', Acquisitions)\n",
    "        if len(AcquisitionNumbers_List) > 0:\n",
    "            AcquisitionNumbers = make_acquisition_numbers(AcquisitionNumbers_List, resource_model, package, dataset, 'e22_AcquisitionNumbers')\n",
    "            AcquisitionNumbers_List = []\n",
    "            print('      - ', AcquisitionNumbers)\n",
    "        if len(AnnotationsMaterial_list) > 0:\n",
    "            AnnotationsMaterial = make_annotations(AnnotationsMaterial_list, resource_model, package, dataset, 'e22_AnnotationsMaterial', 'e22')\n",
    "            AnnotationsMaterial_list =[]\n",
    "            print('      - ', AnnotationsMaterial)    \n",
    "        if len(Archives_list) > 0:\n",
    "            Archives = make_archives(Archives_list, resource_model, package, dataset, 'e22_Archives')\n",
    "            Archives_list = []\n",
    "            print('      - ', Archives)    \n",
    "        if len(Carrier_list) > 0:\n",
    "            Carrier = make_carriers(Carrier_list, resource_model, package, dataset, 'e22_Carrier')\n",
    "            Carrier_list = []\n",
    "            print('      - ', Carrier)    \n",
    "        if len(Conditions_list) > 0:\n",
    "            Conditions = make_conditions(Conditions_list, resource_model, package, dataset, 'e22_Conditions', 'e22')\n",
    "            Conditions_list = []\n",
    "            print('      - ', Conditions)    \n",
    "        if len(Dimensions_list) > 0:\n",
    "            Dimensions = make_dimensions(Dimensions_list, resource_model, package, dataset, 'e22_Dimensions')\n",
    "            Dimensions_list = []\n",
    "            print('      - ', Dimensions)    \n",
    "        if len(DimensionSourceNotes_list) > 0:\n",
    "            DimensionSourceNotes = make_dimension_source_notes(DimensionSourceNotes_list, resource_model, package, dataset, 'e22_DimensionSourceNotes')\n",
    "            DimensionSourceNotes_list = []\n",
    "            print('      - ', DimensionSourceNotes)    \n",
    "        if len(DocumentStatus_list) > 0:\n",
    "            DocumentStatus = e22_make_with_function_and_include_codes(DocumentStatus_list, resource_model, package, dataset, 'e22_DocumentStatus')\n",
    "            DocumentStatus_list = []\n",
    "            print('      - ', DocumentStatus)    \n",
    "        if len(Formats_list) > 0:\n",
    "            Formats = make_formats(Formats_list, resource_model, package, dataset, 'e22_Formats')\n",
    "            Formats_list = []\n",
    "            print('      - ', Formats)    \n",
    "        if len(HallmarkTypes_list) > 0:\n",
    "            HallmarkTypes = e22_make_with_function_and_include_codes(HallmarkTypes_list, resource_model, package, dataset, 'e22_HallmarkTypes')\n",
    "            HallmarkTypes_list = []\n",
    "            print('      - ', HallmarkTypes)    \n",
    "        if len(Items_list) > 0:\n",
    "            Items = make_items(Items_list, resource_model, package, dataset, 'e22_Items')\n",
    "            Items_list = []\n",
    "            print('      - ', Items)    \n",
    "        if len(Marks_list) > 0:\n",
    "            Marks = make_marks(Marks_list, resource_model, package, dataset, 'e22_Marks')\n",
    "            Marks_list = []\n",
    "            print('      - ', Marks)    \n",
    "        if len(Materials_list) > 0:\n",
    "            Materials = make_with_include_codes(Materials_list, resource_model, package, dataset, 'e22_Materials', 'e22')\n",
    "            Materials_list = []\n",
    "            print('      - ', Materials)    \n",
    "        if len(ProductionActors_list) > 0:\n",
    "            ProductionActors = make_plain_ire(ProductionActors_list, resource_model, package, dataset, 'e22_ProductionActors', 'e22')\n",
    "            ProductionActors_list =[]\n",
    "            print('      - ', ProductionActors)    \n",
    "        if len(ProductStadiums_list) > 0:\n",
    "            ProductStadiums = e22_make_with_function_and_include_codes(ProductStadiums_list, resource_model, package, dataset, 'e22_ProductStadiums')\n",
    "            ProductStadiums_list = []\n",
    "            print('      - ', ProductStadiums)    \n",
    "        if len(ProductionTechniques_list) > 0:\n",
    "            ProductionTechniques = make_with_include_codes(ProductionTechniques_list, resource_model, package, dataset, 'e22_ProductionTechniques', 'e22')\n",
    "            ProductionTechniques_list = []\n",
    "            print('      - ', ProductionTechniques)\n",
    "        if len(ExtraProductionTechniques_list) > 0:\n",
    "            ExtraProductionTechniques = make_with_include_codes(ExtraProductionTechniques_list, resource_model, package, dataset, 'e22_ExtraProductionTechniques', 'e22')\n",
    "            ExtraProductionTechniques_list = []\n",
    "            print('      - ', ExtraProductionTechniques)\n",
    "        if len(Single_list) > 0:\n",
    "            Single = make_single(Single_list, resource_model, package, dataset, 'e22_Single')\n",
    "            print('      - ', Single)\n",
    "        if len(ObjectTypes_List) >= 0:\n",
    "            ObjectTypes = make_object_types(Single_list, ObjectTypes_List, resource_model, package, dataset, 'e22_ObjectTypes')\n",
    "            Single_list = []\n",
    "            ObjectTypes_List = []\n",
    "            print('      - ', ObjectTypes)                                    \n",
    "        if len(OtherIdentifiers_list) >= 0:\n",
    "            OtherIdentifiers = make_other_identifier(OtherIdentifiers_list, resource_model, package, dataset, 'e22_OtherIdentifiers')\n",
    "            OtherIdentifiers_list = []\n",
    "            print('      - ', OtherIdentifiers)                                    \n",
    "        if len(ProductionTechniqueNote_list) >= 0:\n",
    "            ProductionTechniqueNote = make_production_technique_note(ProductionTechniqueNote_list, resource_model, package, dataset, 'e22_ProductionTechniqueNote')\n",
    "            ProductionTechniqueNote_list = []\n",
    "            print('      - ', ProductionTechniqueNote)                                    \n",
    "\n",
    "    #DamsLinks = make_dams(resource_model, package, dataset)\n",
    "    #print('      - ', DamsLinks)   \n",
    "       \n",
    "          \n",
    "print('---------') \n",
    "print(datetime.datetime.now())\n",
    "%reset -f \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb5ea82-b03b-4c89-bd67-92b8d2ac6332",
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc62808-2b5c-450e-a7ef-c4d8fd6fbd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
